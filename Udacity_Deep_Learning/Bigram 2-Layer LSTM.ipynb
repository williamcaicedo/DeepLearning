{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique bigrams: 729\n",
      "Most common bigrams (+UNK) [['UNK', 0], ('e ', 3686256), (' t', 2449254), ('s ', 2222333), ('th', 1980538), (' a', 1846251), ('in', 1717706), ('n ', 1654644), ('he', 1597274), ('er', 1549252), (' o', 1468881), ('d ', 1442521), ('on', 1430301), (' s', 1296911), ('an', 1293588), ('t ', 1254402), ('re', 1121783), (' i', 1066337), ('ne', 1032713), ('r ', 1006225)]\n",
      "Sample data [5, 14, 97, 31, 218, 79, 75, 33, 267, 92]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "def read_data_as_bigrams(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of bigrams\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    text = tf.compat.as_str(f.read(f.namelist()[0]))\n",
    "    data = []\n",
    "    lenght = len(text)\n",
    "    #create bigrams\n",
    "    for i in range(lenght - 1):\n",
    "        data.append(text[i] + text[i+1])\n",
    "  return data\n",
    "\n",
    "def build_bigram_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(50000 - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "words = read_data_as_bigrams('text8.zip')\n",
    "data, count, bigram_dictionary, bigram_reverse_dictionary = build_bigram_dataset(words)\n",
    "bigram_vocabulary_size = len(count)\n",
    "print(\"Number of unique bigrams: {0}\".format(len(count)))\n",
    "print('Most common bigrams (+UNK)', count[:20])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocate social relations b', 'nomination gore s endorsement of dean was ', 'when military governments failed to revive', ' three nine one six zero two zero zero one', 'lleria arches national park photographic v', 'reviated as dr mr and mrs respectively the', ' abbeys and monasteries index sacred desti', 'shing the right of appeal to the judicial ', 'married urraca princess of castile daughte', 'sity upset the devils which cost the schoo', 'hel and richard baer h provided a detailed', 'ased in the st family here they are in rou', 'y and liturgical language among jews manda', ' disgust because of the relationship betwe', 'ay opened for passengers in december one n', 'society and that this neglect is the true ', 'tion from the national media and from pres', 'ago based chess records label the influenc', 'migration took place during the one nine e', ' zero zero five yaniv shaked and avishai w', 'new york other well known manufacturers of', 'short subject college humor one nine three', 'he boeing seven six seven a widebody jet w', 'sgow two young white men whose murderers w', 'e listed with a gloss covering some of the', 'lt during this period however the iran ira', 'eber has probably been one of the most inf', ' not dead naturally and hangs herself upon', 'o be made to recognize single acts of meri', 'll s enthusiastic backing darwin read his ', 'yer who received the first card from the d', 'operates three submarines based in talcahu', 'ore significant than in jersey and guernse', 'rmines security of the system provided tha', 'a fierce critic of the poverty and social ', ' fuel extracted from the ground by undergr', ' two six eight in signs of humanity vol th', 'ature that was attacking his livestock it ', 'aristotle s uncaused cause so aquinas come', 'e dragas constantine i of imereti constant', 'ity can be lost as in denaturalization and', 'ecombinant region and the diode becomes co', ' and intracellular ice formation solution ', 'tensive manufacturing sectors the question', 'tion of the size of the input usually meas', 'he attack from hyrsyl northwards and reach', 'dy to pass him a stick to pull him out but', 'ed to bring good fortune to those who carr', 'f certain drugs confusion inability to ori', 'french jansenist theologian b one six thre', 'at it will take to complete an operation c', 'tion from euclidean geometry and analysis ', 'e convince the priest of the mistakes of a', 'ither spontaneously or have employed in th', 'ent told him to name it fort des moines th', 'argest partner of the uk has also made it ', 'ampaign and barred attempts by his opponen', 'ce in a special cell named down s cell the', 'rver side standard formats for mailboxes i', 'gain the amplified signal from q one is di', 'ious texts such as esoteric christianity a', ' assignment of numbers to positions a play', 'o capitalize on the growing popularity of ', 'rettas francis poulenc jean philippe ramea']\n",
      "[' based upon voluntary association of auton', 's helpful to the latter in legitimizing hi', 've the economy and suppress escalating ter', 'ne census peterhead is the largest town in', ' virtual tour of arches national park arch', 'hey are also frequently written as in cana', 'tinations abbeys of france sacred destinat', 'l committee of the privy council an evolvi', 'ter of alfonso viii king of castile and le', 'ool its national ranking the wins over was', 'ed description of the camp s workings duri', 'ough chronological order after the origina', 'daeans and some christians and is still sp', 'ween the anus and feces however it is not ', ' nine zero two on the night of friday one ', 'e cause of the poverty and misery experien', 'esidential candidate john f kennedy despit', 'nce of blues on mainstream american popula', ' eight zero s with the arrival of thousand', ' wool published the paper cracking the blu', 'of bass amplifiers or loudspeakers include', 'ee three too much harmony one nine three t', ' was introduced at around the same time as', ' were asian and whose murders the bnp main', 'heir deeds a significance is attached to t', 'raq war of the one nine eight zero s was a', 'nfluential users of the word in its social', 'on hearing the news discovery and translat', 'rit or meritorious service the required ac', 's first paper to the geological society of', ' deal may be known as eldest hand or as fo', 'huano air force fach gen osvaldo sarabia h', 'sey has maintained light industry as a hig', 'hat there is no analytic attack i e a stru', 'l stratification of victorian society thro', 'ground mining or open pit mining strip min', 'three michel balat and janice deledalle rh', 't was later determined to be a canine most', 'mes to the same conclusion that god exists', 'ntine iii of rome constantine mavrocordato', 'nd gained as in naturalization supranation', 'conductive which allows electrons to flow ', 'n effects are caused by concentration of s', 'on of why the maritimes fell from being a ', 'asured in bits using the most efficient al', 'ched petrozavodsk railroad and the main ro', 'ut she refuses unless he declare his devot', 'rried them ownership was restricted among ', 'rient oneself later signs lethargy decreas', 'ree four one seven two three philip ii duk', ' cannot be bounded in advance see unbounde', 's such as gradient of a function divergenc', ' a pious life the novel the one two zero d', 'their daily writing to explore themselves ', 'the original origin of the name des moines', 't a destination for economic migrants from', 'ents to run campaign advertisements for th', 'he cell is connected to a battery allowing', ' include maildir and mbox several prominen', 'directly fed to the second stage q three w', ' and the work of g i gurdjieff a variety o', 'ayer who is not wearing a number that corr', 'f disco with the album discovery or disco ', 'eau maurice ravel claude joseph rouget de ']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=20\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // (2*batch_size)\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    \n",
    "    # Generate a batch taking only two chars (bigram) from each segment. Next batch will take the\n",
    "    # following chars in each segment and so on.\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      # We lookup the corresponding bigram id and add it to the batch\n",
    "      batch[b] = bigram_dictionary[self._text[self._cursor[b]]+self._text[self._cursor[b] + 1]] \n",
    "      # Increment segment cursor +2 so the next batch picks the next bigram of each segment\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def bigrams(ids):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  bigrams back into its (most likely) character representation.\"\"\"\n",
    "  return [bigram_reverse_dictionary[c] for c in ids]\n",
    "\n",
    "def bigram_batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  \n",
    "  # To reconstruct a word, we need to pick the bigrams in the same position of each batch\n",
    "  # and concatenate them.\n",
    "  # On the first iteration, the comprehension simply transforms ids into bigrams and \n",
    "  # copies them into s. On every subsequent iteration the comprehension will concatenate\n",
    "  # what's already in s with what's in the next batch in the same position.\n",
    "  # In this way it reconstruct the words by joining together the bigrams that were in \n",
    "  # different batches\n",
    "  # We have n batches with m bigrams on each. This will generate m strings with n bigrams on each\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    \n",
    "    s = [''.join(x) for x in zip(s, bigrams(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "print(bigram_batches2string(train_batches.next()))\n",
    "print(bigram_batches2string(train_batches.next()))\n",
    "print(bigram_batches2string(valid_batches.next()))\n",
    "print(bigram_batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample_bigram(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, bigram_vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def bigram_random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, bigram_vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "  \n",
    "  # Parameters:\n",
    "  x = dict()\n",
    "  m = dict()\n",
    "  bl = dict()\n",
    "  # Layer 1\n",
    "  x[\"layer 1\"] = tf.Variable(tf.truncated_normal([bigram_vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  m[\"layer 1\"] = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bl[\"layer 1\"] = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  # Layer 2\n",
    "  x[\"layer 2\"] = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  m[\"layer 2\"] = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bl[\"layer 2\"] = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings for each layer.\n",
    "  saved_output = dict()\n",
    "  saved_state = dict()\n",
    "  # Layer 1\n",
    "  saved_output[\"layer 1\"] = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state[\"layer 1\"] = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Layer 2\n",
    "  saved_output[\"layer 2\"] = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state[\"layer 2\"] = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, layer):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    # the lookup is equivalent to matrix multipication between x and a one-hot encoded input vector\n",
    "    if layer == \"layer 1\":\n",
    "      lstm_tensor = tf.nn.embedding_lookup(x[layer], i) + tf.matmul(o, m[layer]) + bl[layer]\n",
    "    else:\n",
    "      lstm_tensor = tf.matmul(i, x[layer]) + tf.matmul(o, m[layer]) + bl[layer]\n",
    "    \n",
    "    input_gate = tf.sigmoid(lstm_tensor[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(lstm_tensor[:, num_nodes:2*num_nodes])\n",
    "    update = lstm_tensor[:, 2*num_nodes:3*num_nodes]\n",
    "    output_gate = tf.sigmoid(lstm_tensor[:, 3*num_nodes:])\n",
    "    \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data. Now instead of one-hot encoding vectors, we only take the bigram id as input\n",
    "  train_data = list()\n",
    "  #train_labels = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    \n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "    \n",
    "  identity = tf.constant(np.identity(bigram_vocabulary_size, dtype = np.int32))\n",
    "  train_labels = [tf.nn.embedding_lookup(identity, td) for td in train_data[1:]]\n",
    "    \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output[\"layer 1\"]\n",
    "  state = saved_state[\"layer 1\"]\n",
    "  output_layer_2 = saved_output[\"layer 2\"]\n",
    "  state_layer_2 = saved_state[\"layer 2\"]\n",
    "    \n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state, \"layer 1\")\n",
    "    #outputs.append(tf.nn.dropout(output, keep_prob=0.8))\n",
    "    output_layer_2, state_layer_2 = lstm_cell(output, output_layer_2, state_layer_2, \"layer 2\")\n",
    "    outputs.append(output_layer_2)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output[\"layer 1\"].assign(output),\n",
    "                                saved_state[\"layer 1\"].assign(state),\n",
    "                                saved_output[\"layer 2\"].assign(output_layer_2),\n",
    "                                saved_state[\"layer 2\"].assign(state_layer_2)]):\n",
    "    \n",
    "    \n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = dict()  \n",
    "  saved_sample_state = dict()\n",
    "  saved_sample_output[\"layer 1\"] = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state[\"layer 1\"] = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_output[\"layer 2\"] = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state[\"layer 2\"] = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output[\"layer 1\"].assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state[\"layer 1\"].assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_output[\"layer 2\"].assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state[\"layer 2\"].assign(tf.zeros([1, num_nodes])))\n",
    "\n",
    "  sample_output = dict()\n",
    "  sample_state = dict()\n",
    "  sample_output[\"layer 1\"], sample_state[\"layer 1\"] = lstm_cell(\n",
    "    sample_input, saved_sample_output[\"layer 1\"], saved_sample_state[\"layer 1\"], \"layer 1\")\n",
    "    \n",
    "  sample_output[\"layer 2\"], sample_state[\"layer 2\"] = lstm_cell(\n",
    "    sample_output[\"layer 1\"], saved_sample_output[\"layer 2\"], saved_sample_state[\"layer 2\"], \"layer 2\")\n",
    "\n",
    "  with tf.control_dependencies([saved_sample_output[\"layer 1\"].assign(sample_output[\"layer 1\"]),\n",
    "                                saved_sample_state[\"layer 1\"].assign(sample_state[\"layer 1\"]),\n",
    "                                saved_sample_output[\"layer 2\"].assign(sample_output[\"layer 2\"]),\n",
    "                                saved_sample_state[\"layer 2\"].assign(sample_state[\"layer 2\"])]):\n",
    "        \n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output[\"layer 2\"], w, b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.619672 learning rate: 10.000000\n",
      "Minibatch perplexity: 749.70\n",
      "========================================\n",
      "dzye llemr cnreb hdlvpjrrlxsqxhccbzguauwisxartequjsobozzfhcrxrtwhntibjzsavpqzdlm\n",
      "t unfhjclzgmzqnwgtjvslq yvavgbtswdwqualefikqmmfrtuszzuvtykpbygkorgezvlaqhucb bcu\n",
      "kfp hesssc kpgkiqopdbrumurmhzwliarssy sfkkfrqd ysvlzaptyvfmoabkcuipjlmede jarneb\n",
      "dsfv kmuaxmxfehypmrqyvgpovrmgdazlocvjz miikumpqnbkonhsmtdtmjqghheewrxyasnudocqef\n",
      "ijvmspdatlvppec  ehygqowofxtzafzvruwvwndoxasmcikgazyexyiehhanmfgdubun eiyufjoezo\n",
      "========================================\n",
      "Validation set perplexity: 647.07\n",
      "Average loss at step 100: 5.544343 learning rate: 10.000000\n",
      "Minibatch perplexity: 203.47\n",
      "Validation set perplexity: 206.12\n",
      "Average loss at step 200: 5.316868 learning rate: 10.000000\n",
      "Minibatch perplexity: 215.71\n",
      "Validation set perplexity: 193.36\n",
      "Average loss at step 300: 5.286790 learning rate: 10.000000\n",
      "Minibatch perplexity: 193.83\n",
      "Validation set perplexity: 189.73\n",
      "Average loss at step 400: 5.266559 learning rate: 10.000000\n",
      "Minibatch perplexity: 179.09\n",
      "Validation set perplexity: 184.91\n",
      "Average loss at step 500: 5.233452 learning rate: 10.000000\n",
      "Minibatch perplexity: 181.66\n",
      "Validation set perplexity: 177.17\n",
      "Average loss at step 600: 5.231662 learning rate: 10.000000\n",
      "Minibatch perplexity: 190.90\n",
      "Validation set perplexity: 181.82\n",
      "Average loss at step 700: 5.226989 learning rate: 10.000000\n",
      "Minibatch perplexity: 185.86\n",
      "Validation set perplexity: 173.87\n",
      "Average loss at step 800: 5.187575 learning rate: 10.000000\n",
      "Minibatch perplexity: 193.14\n",
      "Validation set perplexity: 167.59\n",
      "Average loss at step 900: 5.155015 learning rate: 10.000000\n",
      "Minibatch perplexity: 165.88\n",
      "Validation set perplexity: 159.95\n",
      "Average loss at step 1000: 5.071222 learning rate: 10.000000\n",
      "Minibatch perplexity: 161.40\n",
      "========================================\n",
      "cihe zt theee whesityao erro danenflnlore o roris care mbesins aauorilc ntot bti\n",
      "oulek esndthalrmti s rte sargror biohistbibir prctannddyr  eale bonet hetiinngum\n",
      " don thearidriing redeisin dornsr rawaarpea n sts onri mjdmyyeon tntmainryanosee\n",
      "cd aedit b m ameergea in a mzeonct norncan sepurutocmmelalons  squm coer sret  b\n",
      "lee ncte slly raou son at simeroesatf idsto ndfoacons bea masitiarnelae onnee ph\n",
      "========================================\n",
      "Validation set perplexity: 150.36\n",
      "Average loss at step 1100: 4.981912 learning rate: 10.000000\n",
      "Minibatch perplexity: 133.13\n",
      "Validation set perplexity: 139.56\n",
      "Average loss at step 1200: 4.851016 learning rate: 10.000000\n",
      "Minibatch perplexity: 117.58\n",
      "Validation set perplexity: 139.08\n",
      "Average loss at step 1300: 4.796312 learning rate: 10.000000\n",
      "Minibatch perplexity: 121.11\n",
      "Validation set perplexity: 121.78\n",
      "Average loss at step 1400: 4.726132 learning rate: 10.000000\n",
      "Minibatch perplexity: 106.09\n",
      "Validation set perplexity: 120.43\n",
      "Average loss at step 1500: 4.648412 learning rate: 10.000000\n",
      "Minibatch perplexity: 98.62\n",
      "Validation set perplexity: 105.87\n",
      "Average loss at step 1600: 4.543938 learning rate: 10.000000\n",
      "Minibatch perplexity: 92.67\n",
      "Validation set perplexity: 94.69\n",
      "Average loss at step 1700: 4.450233 learning rate: 10.000000\n",
      "Minibatch perplexity: 84.82\n",
      "Validation set perplexity: 90.41\n",
      "Average loss at step 1800: 4.343243 learning rate: 10.000000\n",
      "Minibatch perplexity: 83.88\n",
      "Validation set perplexity: 86.84\n",
      "Average loss at step 1900: 4.284293 learning rate: 10.000000\n",
      "Minibatch perplexity: 67.26\n",
      "Validation set perplexity: 77.35\n",
      "Average loss at step 2000: 4.220857 learning rate: 10.000000\n",
      "Minibatch perplexity: 62.32\n",
      "========================================\n",
      "aa so in acoten and slar b at ernturat foond ppponand aslf fena or amser of alte\n",
      "yg nhetg kianttrorliry arcxedeionaemer in a dirr the eoxhan fran momooy mbhy o a\n",
      "rrupt momet nod for fasingloo as gthes of fones in a ptheinored to gus one srade\n",
      "vkes and two ming raazm scrot kins geelfl on the pogrited mor yucoy and one eigh\n",
      "iy ber one of davivil eriem one j rrur in bode the betd fantbl lore the sust pko\n",
      "========================================\n",
      "Validation set perplexity: 75.03\n",
      "Average loss at step 2100: 4.171708 learning rate: 10.000000\n",
      "Minibatch perplexity: 59.77\n",
      "Validation set perplexity: 73.06\n",
      "Average loss at step 2200: 4.095564 learning rate: 10.000000\n",
      "Minibatch perplexity: 56.76\n",
      "Validation set perplexity: 67.61\n",
      "Average loss at step 2300: 4.071661 learning rate: 10.000000\n",
      "Minibatch perplexity: 56.47\n",
      "Validation set perplexity: 63.43\n",
      "Average loss at step 2400: 4.057394 learning rate: 10.000000\n",
      "Minibatch perplexity: 58.64\n",
      "Validation set perplexity: 66.64\n",
      "Average loss at step 2500: 4.005829 learning rate: 10.000000\n",
      "Minibatch perplexity: 54.96\n",
      "Validation set perplexity: 58.72\n",
      "Average loss at step 2600: 3.912015 learning rate: 10.000000\n",
      "Minibatch perplexity: 49.94\n",
      "Validation set perplexity: 60.56\n",
      "Average loss at step 2700: 3.906518 learning rate: 10.000000\n",
      "Minibatch perplexity: 51.76\n",
      "Validation set perplexity: 55.64\n",
      "Average loss at step 2800: 3.870628 learning rate: 10.000000\n",
      "Minibatch perplexity: 50.47\n",
      "Validation set perplexity: 51.40\n",
      "Average loss at step 2900: 3.831603 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.95\n",
      "Validation set perplexity: 49.57\n",
      "Average loss at step 3000: 3.781676 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.76\n",
      "========================================\n",
      "pdina lasompating eceaor would one nine brtree mor vier preach rlaz ticth ground\n",
      "xphuks oth etrantic may al thas hermy in o the fore deviding megp as a chorcy cu\n",
      "skrat in milthar one nine hethuling from nolter everh both or pi on bwtaus one n\n",
      "ls there of using is banstentication theor kane bances the gound of the neen o r\n",
      "qnpok reoring the paw americn one jangring in ladetatacal in lave of a worklebpl\n",
      "========================================\n",
      "Validation set perplexity: 46.83\n",
      "Average loss at step 3100: 3.738879 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.50\n",
      "Validation set perplexity: 44.31\n",
      "Average loss at step 3200: 3.662234 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.97\n",
      "Validation set perplexity: 43.57\n",
      "Average loss at step 3300: 3.675842 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.24\n",
      "Validation set perplexity: 40.71\n",
      "Average loss at step 3400: 3.652846 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.70\n",
      "Validation set perplexity: 40.14\n",
      "Average loss at step 3500: 3.636659 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.49\n",
      "Validation set perplexity: 37.10\n",
      "Average loss at step 3600: 3.644687 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.75\n",
      "Validation set perplexity: 36.66\n",
      "Average loss at step 3700: 3.613927 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.85\n",
      "Validation set perplexity: 36.56\n",
      "Average loss at step 3800: 3.583390 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.30\n",
      "Validation set perplexity: 35.79\n",
      "Average loss at step 3900: 3.566234 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.05\n",
      "Validation set perplexity: 33.72\n",
      "Average loss at step 4000: 3.548373 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.92\n",
      "========================================\n",
      "zatus tous seven the of map with muvde one nine six three was other is and nase \n",
      "by in supere of graph morol as a hesps sugech the most examporrate fas aimodic i\n",
      " pencordising ecest zofe have doacy beair heodery to bee gace imatelath when tha\n",
      " the sopon and to obvelly but be been staircank mectian to brirwetsh in the basq\n",
      "jim be consuated poee of hisand ad other called sering she motaw kpebrapors depe\n",
      "========================================\n",
      "Validation set perplexity: 31.52\n",
      "Average loss at step 4100: 3.529162 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.79\n",
      "Validation set perplexity: 30.43\n",
      "Average loss at step 4200: 3.512680 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.84\n",
      "Validation set perplexity: 30.25\n",
      "Average loss at step 4300: 3.496517 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.86\n",
      "Validation set perplexity: 29.16\n",
      "Average loss at step 4400: 3.478593 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.97\n",
      "Validation set perplexity: 28.70\n",
      "Average loss at step 4500: 3.454365 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.70\n",
      "Validation set perplexity: 28.41\n",
      "Average loss at step 4600: 3.397385 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.13\n",
      "Validation set perplexity: 27.40\n",
      "Average loss at step 4700: 3.364634 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.58\n",
      "Validation set perplexity: 27.20\n",
      "Average loss at step 4800: 3.385167 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.68\n",
      "Validation set perplexity: 28.08\n",
      "Average loss at step 4900: 3.385728 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.86\n",
      "Validation set perplexity: 27.91\n",
      "Average loss at step 5000: 3.358605 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.64\n",
      "========================================\n",
      "frhe u qain jordary rive tave bonupon seigin a bescounds moro breaghte in study \n",
      "jm cu reallvy romantically the urt howeverr decomestujaphirad learits to peone i\n",
      "s aqi trieg to restrion it is a servics is the kinicion pefflem ruotia g three z\n",
      "oegly one nine ns five one three b otceewn been were usually ogs orcocation of o\n",
      "tnicall the loam a gropoer memmed to in south to the the q wither allelf on runt\n",
      "========================================\n",
      "Validation set perplexity: 26.92\n",
      "Average loss at step 5100: 3.332205 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.92\n",
      "Validation set perplexity: 25.52\n",
      "Average loss at step 5200: 3.314301 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.19\n",
      "Validation set perplexity: 25.28\n",
      "Average loss at step 5300: 3.302859 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.30\n",
      "Validation set perplexity: 24.66\n",
      "Average loss at step 5400: 3.299112 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.60\n",
      "Validation set perplexity: 24.21\n",
      "Average loss at step 5500: 3.263380 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.82\n",
      "Validation set perplexity: 24.02\n",
      "Average loss at step 5600: 3.304820 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.16\n",
      "Validation set perplexity: 23.77\n",
      "Average loss at step 5700: 3.342256 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.67\n",
      "Validation set perplexity: 23.39\n",
      "Average loss at step 5800: 3.360196 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.44\n",
      "Validation set perplexity: 23.27\n",
      "Average loss at step 5900: 3.352748 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.74\n",
      "Validation set perplexity: 22.79\n",
      "Average loss at step 6000: 3.305229 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.82\n",
      "========================================\n",
      "jzan shecishes any one ccolors ixcecithy and the the vemey moverretents eight ei\n",
      "nsting against blemeral theory k morore drchivish forms educy mating in the olde\n",
      "rpemer and from his stalary connistity and hseas on the sama converbed with and \n",
      "sqo the united in withintal vea by lary to he mecrame word hasest stanring ran b\n",
      "tv food frence or morepond greocesiqui micean and placying forse that on vicated\n",
      "========================================\n",
      "Validation set perplexity: 22.49\n",
      "Average loss at step 6100: 3.284790 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.21\n",
      "Validation set perplexity: 22.32\n",
      "Average loss at step 6200: 3.287974 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.36\n",
      "Validation set perplexity: 22.52\n",
      "Average loss at step 6300: 3.296470 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.40\n",
      "Validation set perplexity: 22.38\n",
      "Average loss at step 6400: 3.256130 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.16\n",
      "Validation set perplexity: 22.18\n",
      "Average loss at step 6500: 3.293993 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.95\n",
      "Validation set perplexity: 22.19\n",
      "Average loss at step 6600: 3.261447 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.85\n",
      "Validation set perplexity: 21.95\n",
      "Average loss at step 6700: 3.277356 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.05\n",
      "Validation set perplexity: 21.66\n",
      "Average loss at step 6800: 3.314310 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.94\n",
      "Validation set perplexity: 22.07\n",
      "Average loss at step 6900: 3.310105 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.97\n",
      "Validation set perplexity: 21.94\n",
      "Average loss at step 7000: 3.271092 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.49\n",
      "========================================\n",
      "aade stown as terms the the bard of the detch and cuntall rhostian yess recentio\n",
      "aferded webn part opany the one nine two nine four four zero six and law viffere\n",
      "ual being the rates frad the gour this majors leagened conese a souamuno one the\n",
      "fkarhuy throioments compached at the bohlipmeng buedo depergeded common marred b\n",
      "vninment begintian plactas if mocre low five seven three two four zero zero zero\n",
      "========================================\n",
      "Validation set perplexity: 21.99\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    \n",
    "    _, l, predictions, lr, batch_labels = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate, train_labels], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, np.reshape(batch_labels,[-1, bigram_vocabulary_size])))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 40)\n",
    "        for _ in range(5):\n",
    "          feed = sample_bigram(bigram_random_distribution())\n",
    "          sentence = bigrams([np.argmax(feed)])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(39):\n",
    "            prediction = sample_prediction.eval({sample_input: [np.argmax(feed)]})\n",
    "            feed = sample_bigram(prediction)\n",
    "            sentence += bigrams([np.argmax(feed)])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 40)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_label = np.zeros([1, bigram_vocabulary_size])\n",
    "        valid_label[0,int(b[1])] = 1\n",
    "        valid_logprob = valid_logprob + logprob(predictions, valid_label)\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
