{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "## Assignment 6\n",
    "\n",
    "After training a skip-gram model in 5_word2vec.ipynb, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    \n",
    "    # Generate a batch taking only one char from each segment. Next batch will take the\n",
    "    # following char in each segment and so on.\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      # use the char id to set the corresponding position in the row to 1\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0 \n",
    "      # increment segment cursor +1 so the next batch picks the next char of each segment\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  \n",
    "  # To reconstruct a word, we need to pick the char in the same position of each batch\n",
    "  # and concatenate them.\n",
    "  # On the first iteration, the comprehension simply transforms ids into chars and \n",
    "  # copies them into s. On every subsequent iteration the comprehension will concatenate\n",
    "  # what's already in s with what's in the next batch in the same position.\n",
    "  # In this way it reconstruct the words by joining together the chars that were in \n",
    "  # different batches\n",
    "  # We have n batches with m chars on each. This will generate m strings with n chars on each\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    \n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  '''\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  '''\n",
    "  x = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  m = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bl = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    '''\n",
    "    x = tf.concat([ix, fx, cx, ox], 1)\n",
    "    m = tf.concat([im, fm, cm, om], 1)\n",
    "    b = tf.concat([ib, fb, cb, ob], 1)\n",
    "    '''\n",
    "    lstm_tensor = tf.matmul(i, x) + tf.matmul(o, m) + bl\n",
    "    \n",
    "    input_gate = tf.sigmoid(lstm_tensor[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(lstm_tensor[:, num_nodes:2*num_nodes])\n",
    "    update = lstm_tensor[:, 2*num_nodes:3*num_nodes]\n",
    "    output_gate = tf.sigmoid(lstm_tensor[:, 3*num_nodes:])\n",
    "    \n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294177 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "mr z mjxnljteaszxacg tpiiocht bbfk ee diaewmsecwahodfanofhepsihheuur  suidasegse\n",
      "cpq vovb cvfn w bwuloipimxtizmp ots kei wmthtncv zaai bdbjiv dljrnne eld giltyge\n",
      "yf  eponp inioakodvnanhejpl rsybaenhtuyeqnnkqvap gzcngn oateedoysbryt fudneascih\n",
      "jwa nnee vrhbnuieahetw t zpixch pgf  re oiorea ct nae   ye sec r ykrad tsskoo er\n",
      "h jiwezohxxtkkysnczizetl egr xeotslslc  nxtdczzk nsf n kdi epbjjbay jaijneat wn \n",
      "================================================================================\n",
      "Validation set perplexity: 19.91\n",
      "Average loss at step 100: 2.581781 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.63\n",
      "Validation set perplexity: 10.40\n",
      "Average loss at step 200: 2.241921 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.29\n",
      "Validation set perplexity: 8.66\n",
      "Average loss at step 300: 2.105236 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.31\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 400: 2.016612 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 500: 1.964071 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.85\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 600: 1.903283 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 700: 1.868356 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 800: 1.854838 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 900: 1.817347 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1000: 1.819508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "================================================================================\n",
      "y as pafe enceed as workd the eaking hy as wergl him calia stagagitaza impeadati\n",
      "y siqual changa city the wellity aciet the infaller contermation fan the schorse\n",
      "ca essstrae work and parol in the praces as kewalement nit stal nigar iplear his\n",
      "jet live no pay bwousb of his canata ech at canic seaklical as is six povulity a\n",
      "vhers with lisuig intervity sch monty in ssingh fas fastic stlae leser anve reci\n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1100: 1.777475 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1200: 1.769470 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1300: 1.770505 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1400: 1.761505 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1500: 1.754020 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1600: 1.735598 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1700: 1.734825 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1800: 1.722687 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1900: 1.727602 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2000: 1.716981 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "fold and the gasts heal actives depein and the partable gag an ampreaten sins th\n",
      "d in niton of annen tales wire war and singonies gologwy aurrakings lene of the \n",
      "der bwv two seven seven vowayous shownence in one hishage to whoch with to estot\n",
      "jaman christitio of wied grannement levie manerm and previne the universides to \n",
      "ure fietendy arts in foo the invo nine four but cespor latomative an a ningose s\n",
      "================================================================================\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2100: 1.705241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2200: 1.719435 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2300: 1.717573 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2400: 1.720148 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2500: 1.714806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2600: 1.686329 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2700: 1.661459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2800: 1.660202 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2900: 1.672164 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3000: 1.680818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "================================================================================\n",
      "tilition i moder the mointuritis parts the epalos stomevade exteen of drow by fa\n",
      "ani vieor of those and small other corined that the internai beor withownorly ab\n",
      "rigated the s wernemong is barded catablegs atsime this which nates on the schep\n",
      "kas unual one five five nine nine eight eight eight eight s seven seven the char\n",
      "olos prictive americanmentay population pointed without alpucemic the strangutio\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3100: 1.665534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3200: 1.669457 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3300: 1.680830 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3400: 1.660886 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3500: 1.643322 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3600: 1.666988 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 3700: 1.651827 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3800: 1.623454 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3900: 1.629499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4000: 1.628359 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "with thus heseder some centralin and bangeses was leck lass three edan unhauphor\n",
      "ched so bavelish education part proces agrissus of as a time of kean bilf this n\n",
      "was eking is somical for and eke loss and only as the unknom alonghounturals sta\n",
      "kings from albuiloval new pauds of a detain the the markers cerment complectures\n",
      "um duading has esshorf almught corned and viducess explacer than sachical see ad\n",
      "================================================================================\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4100: 1.622916 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4200: 1.630631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4300: 1.610235 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4400: 1.607731 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4500: 1.601687 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4600: 1.627991 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4700: 1.628894 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4800: 1.639709 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4900: 1.625658 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5000: 1.651280 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "d and of the oregrande contember einee saccolondon to an adciscytoty musestist m\n",
      "ge sective tallead of the ring experie periracter list mecker anquader used batt\n",
      "s of a several as been pamains in grankerial as that resprialhy freedeat of tone\n",
      "jaliates keyboser aftern the mixstiturged t edia lagiprative of fill with own ju\n",
      "x all the echanstaste of maol quecture adut marions a hyse with of theory bestri\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5100: 1.610713 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5200: 1.585213 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5300: 1.607165 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5400: 1.602772 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5500: 1.593219 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5600: 1.596894 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5700: 1.587693 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5800: 1.577796 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5900: 1.584914 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6000: 1.574092 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "ine after calueb to gast pa an annilled began unxiting coppanian corcorghtly of \n",
      "dial of have system ton a d cooporantioned the univer in charrign of hole is d f\n",
      "s if the on anilobally supe cr muterest of at the openity frencic to levelly sue\n",
      "fortation revandar geiphish of stran paysic penter hangures used haldrest chickl\n",
      "water per chishesic to sought mypr ecorourection of netwoeld ratient and blackso\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6100: 1.601696 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6200: 1.598822 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6300: 1.555477 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6400: 1.578717 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6500: 1.596733 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6600: 1.590932 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6700: 1.597698 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6800: 1.592554 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6900: 1.583570 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 7000: 1.592259 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "================================================================================\n",
      " composers avousher in estabtion of as singenderact of despontan are was monethe\n",
      "us than that rise senance and regions foderber bothk the kchange when the found \n",
      "poing treater attractities shortfrite of olkism was achievist a act but milite c\n",
      "gions in a general bow sumport of operation nee a statear ail other by each as t\n",
      "go sian boy the and have or eacturent reallers one nune not te keart a discarged\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "x = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "m = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "bl = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    lstm_tensor = tf.matmul(i, x) + tf.matmul(o, m) + bl\n",
    "    \n",
    "    input_gate = tf.sigmoid(lstm_tensor[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(lstm_tensor[:, num_nodes:2*num_nodes])\n",
    "    update = lstm_tensor[:, 2*num_nodes:3*num_nodes]\n",
    "    output_gate = tf.sigmoid(lstm_tensor[:, 3*num_nodes:])\n",
    "    \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state ```\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique bigrams: 729\n",
      "Most common words (+UNK) [['UNK', 0], ('e ', 3686256), (' t', 2449254), ('s ', 2222333), ('th', 1980538), (' a', 1846251), ('in', 1717706), ('n ', 1654644), ('he', 1597274), ('er', 1549252), (' o', 1468881), ('d ', 1442521), ('on', 1430301), (' s', 1296911), ('an', 1293588), ('t ', 1254402), ('re', 1121783), (' i', 1066337), ('ne', 1032713), ('r ', 1006225)]\n",
      "Sample data [5, 14, 97, 31, 218, 79, 75, 33, 267, 92]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "def read_data_as_bigrams(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of bigrams\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    text = tf.compat.as_str(f.read(f.namelist()[0]))\n",
    "    data = []\n",
    "    lenght = len(text)\n",
    "    #create bigrams\n",
    "    for i in range(lenght - 1):\n",
    "        data.append(text[i] + text[i+1])\n",
    "  return data\n",
    "\n",
    "def build_bigram_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(50000 - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "words = read_data_as_bigrams('text8.zip')\n",
    "data, count, bigram_dictionary, bigram_reverse_dictionary = build_bigram_dataset(words)\n",
    "bigram_vocabulary_size = len(count)\n",
    "print(\"Number of unique bigrams: {0}\".format(len(count)))\n",
    "print('Most common words (+UNK)', count[:20])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    \n",
    "    # Generate a batch taking only two chars (bigram) from each segment. Next batch will take the\n",
    "    # following chars in each segment and so on.\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      # We lookup the corresponding bigram id and add it to the batch\n",
    "      batch[b] = bigram_dictionary[self._text[self._cursor[b]]+self._text[self._cursor[b] + 1]] \n",
    "      # Increment segment cursor +2 so the next batch picks the next bigram of each segment\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def bigrams(ids):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  bigrams back into its (most likely) character representation.\"\"\"\n",
    "  return [bigram_reverse_dictionary[c] for c in ids]\n",
    "\n",
    "def bigram_batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  \n",
    "  # To reconstruct a word, we need to pick the bigrams in the same position of each batch\n",
    "  # and concatenate them.\n",
    "  # On the first iteration, the comprehension simply transforms ids into bigrams and \n",
    "  # copies them into s. On every subsequent iteration the comprehension will concatenate\n",
    "  # what's already in s with what's in the next batch in the same position.\n",
    "  # In this way it reconstruct the words by joining together the bigrams that were in \n",
    "  # different batches\n",
    "  # We have n batches with m bigrams on each. This will generate m strings with n bigrams on each\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    \n",
    "    s = [''.join(x) for x in zip(s, bigrams(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "print(bigram_batches2string(train_batches.next()))\n",
    "print(bigram_batches2string(train_batches.next()))\n",
    "print(bigram_batches2string(valid_batches.next()))\n",
    "print(bigram_batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample_bigram(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, bigram_vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def bigram_random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, bigram_vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "  \n",
    "  # Parameters:\n",
    "  \n",
    "  x = tf.Variable(tf.truncated_normal([bigram_vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  m = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bl = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    # the lookup is equivalent to matrix multipication between x and a one-hot encoded input vector\n",
    "    lstm_tensor = tf.nn.embedding_lookup(x, i) + tf.matmul(o, m) + bl\n",
    "    \n",
    "    input_gate = tf.sigmoid(lstm_tensor[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(lstm_tensor[:, num_nodes:2*num_nodes])\n",
    "    update = lstm_tensor[:, 2*num_nodes:3*num_nodes]\n",
    "    output_gate = tf.sigmoid(lstm_tensor[:, 3*num_nodes:])\n",
    "    \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data. Now instead of one-hot encoding vectors, we only take the bigram id as input\n",
    "  train_data = list()\n",
    "  #train_labels = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    \n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "\n",
    "  '''train_labels = list()\n",
    "  for batch in train_data[1:]:\n",
    "    batch_labels = np.zeros([batch_size, vocabulary_size])\n",
    "    for i,bigram_id in enumerate(batch):\n",
    "      bigram = bigrams(bigram_id)\n",
    "      batch_labels[i,char2id([bigram[0]])] = 1\n",
    "    train_labels.append(tf.Variable(batch_labels, dtype=tf.int32, trainable=False))'''\n",
    "    \n",
    "  identity = tf.constant(np.identity(bigram_vocabulary_size, dtype = np.int32))\n",
    "  \n",
    "  train_labels = [tf.nn.embedding_lookup(identity, td) for td in train_data[1:]]\n",
    "    \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(tf.nn.dropout(output, keep_prob=1))\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    \n",
    "    \n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.589625 learning rate: 10.000000\n",
      "Minibatch perplexity: 727.51\n",
      "========================================\n",
      "ips dvze eqqdelzwvizvyoolcgplhcrchlpanpcmrumpjhs awlidfznsxogdjankpnntbvtt yqtcb\n",
      "awuuy bpqfxjkuskrfmhokztsuywlihi ke gjpmxxxqpzeipqqrpjcfrsrmivwxyzmmmlli uzq ceg\n",
      "fkvz gaqajnovkewxk mshbswshvomtdyafulwzpxouhefgmntccxhkumsiycuusfgsludzgqbpq yfv\n",
      " lrkmesurrxsuibklghn orlbooqqgifs exgpptnflbqonbxomupmuajbwjuasemrozrsmvkjp olhl\n",
      "mwftwrttm fdacpgzsmamxodvaxalahyjdrjzxnqycypzkvbykrvlfpzoijwxqrh chxxeoqrgpllxwu\n",
      "========================================\n",
      "Validation set perplexity: 667.75\n",
      "Average loss at step 100: 5.443450 learning rate: 10.000000\n",
      "Minibatch perplexity: 184.79\n",
      "Validation set perplexity: 176.30\n",
      "Average loss at step 200: 5.136605 learning rate: 10.000000\n",
      "Minibatch perplexity: 148.37\n",
      "Validation set perplexity: 158.50\n",
      "Average loss at step 300: 4.835202 learning rate: 10.000000\n",
      "Minibatch perplexity: 114.14\n",
      "Validation set perplexity: 113.93\n",
      "Average loss at step 400: 4.540526 learning rate: 10.000000\n",
      "Minibatch perplexity: 92.69\n",
      "Validation set perplexity: 103.30\n",
      "Average loss at step 500: 4.399908 learning rate: 10.000000\n",
      "Minibatch perplexity: 86.30\n",
      "Validation set perplexity: 87.08\n",
      "Average loss at step 600: 4.197075 learning rate: 10.000000\n",
      "Minibatch perplexity: 73.15\n",
      "Validation set perplexity: 70.50\n",
      "Average loss at step 700: 4.071126 learning rate: 10.000000\n",
      "Minibatch perplexity: 68.30\n",
      "Validation set perplexity: 63.80\n",
      "Average loss at step 800: 4.056749 learning rate: 10.000000\n",
      "Minibatch perplexity: 54.09\n",
      "Validation set perplexity: 58.16\n",
      "Average loss at step 900: 3.924034 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.35\n",
      "Validation set perplexity: 54.55\n",
      "Average loss at step 1000: 3.852884 learning rate: 10.000000\n",
      "Minibatch perplexity: 50.60\n",
      "========================================\n",
      " newd five five biperaefen nsont artinok id to in nine conk of sfbm emsting sunc\n",
      "buantings quterrtima kodg diuncterguo sorals ato atitating actor be hicurming ab\n",
      "ezed bicdevermbbwtsed co the brian the ty queyteaction four bal the sotust byeca\n",
      "prpuwyr vistoptimanting map scmend to oytern eisbeng four six egeclial but paspe\n",
      "den and asanyeor amelicars in the laticiy as oneiac littst fellencical and nelms\n",
      "========================================\n",
      "Validation set perplexity: 53.10\n",
      "Average loss at step 1100: 3.883036 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.93\n",
      "Validation set perplexity: 49.40\n",
      "Average loss at step 1200: 3.779816 learning rate: 10.000000\n",
      "Minibatch perplexity: 54.70\n",
      "Validation set perplexity: 44.29\n",
      "Average loss at step 1300: 3.774088 learning rate: 10.000000\n",
      "Minibatch perplexity: 47.83\n",
      "Validation set perplexity: 40.89\n",
      "Average loss at step 1400: 3.736206 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.15\n",
      "Validation set perplexity: 38.30\n",
      "Average loss at step 1500: 3.700908 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.32\n",
      "Validation set perplexity: 38.80\n",
      "Average loss at step 1600: 3.645913 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.01\n",
      "Validation set perplexity: 38.52\n",
      "Average loss at step 1700: 3.679513 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.49\n",
      "Validation set perplexity: 35.87\n",
      "Average loss at step 1800: 3.664485 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.18\n",
      "Validation set perplexity: 34.57\n",
      "Average loss at step 1900: 3.616078 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.24\n",
      "Validation set perplexity: 33.20\n",
      "Average loss at step 2000: 3.606720 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.12\n",
      "========================================\n",
      "frameath mocease of ease was the state fut findiuring one oftens is heftdarative\n",
      " grespober that three zero zero zero fadw eight one nine one nine foureats one t\n",
      "lqddled and disaegal creigtion of eaimcoboms with rjck sublicady the is orpeatio\n",
      "ive efucisting his larmance archa in of fucracgrand bylso mist tule compraet sim\n",
      " sigte into vised eight inmare of opents whel fevesing may whisterath colerican \n",
      "========================================\n",
      "Validation set perplexity: 32.92\n",
      "Average loss at step 2100: 3.581625 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.17\n",
      "Validation set perplexity: 29.93\n",
      "Average loss at step 2200: 3.534687 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.96\n",
      "Validation set perplexity: 30.54\n",
      "Average loss at step 2300: 3.519638 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.36\n",
      "Validation set perplexity: 31.12\n",
      "Average loss at step 2400: 3.547255 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.12\n",
      "Validation set perplexity: 29.19\n",
      "Average loss at step 2500: 3.506240 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.12\n",
      "Validation set perplexity: 30.15\n",
      "Average loss at step 2600: 3.497621 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.33\n",
      "Validation set perplexity: 28.82\n",
      "Average loss at step 2700: 3.447839 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.26\n",
      "Validation set perplexity: 27.65\n",
      "Average loss at step 2800: 3.429371 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.93\n",
      "Validation set perplexity: 28.87\n",
      "Average loss at step 2900: 3.429039 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.11\n",
      "Validation set perplexity: 28.55\n",
      "Average loss at step 3000: 3.395207 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.65\n",
      "========================================\n",
      "sq ordera de kmal nine can one sucrafey intea lows colrue boy yegy on amerents a\n",
      "hjile has so d one seven zs informed that server ribles and s jelaraga impilian \n",
      "drsle stewary pose a the nine into compautation lina music rowers drro bogition \n",
      "wc distance breal list the no quesch accince the more sourfrone consuntent of co\n",
      "vtmism a faible siviing an a rarhat sometion of jan aused with tradia ber day a \n",
      "========================================\n",
      "Validation set perplexity: 28.83\n",
      "Average loss at step 3100: 3.358208 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.40\n",
      "Validation set perplexity: 28.15\n",
      "Average loss at step 3200: 3.328777 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.32\n",
      "Validation set perplexity: 26.44\n",
      "Average loss at step 3300: 3.384383 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.26\n",
      "Validation set perplexity: 26.35\n",
      "Average loss at step 3400: 3.409295 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.11\n",
      "Validation set perplexity: 25.75\n",
      "Average loss at step 3500: 3.364590 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.46\n",
      "Validation set perplexity: 25.73\n",
      "Average loss at step 3600: 3.347531 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.22\n",
      "Validation set perplexity: 25.21\n",
      "Average loss at step 3700: 3.350509 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.54\n",
      "Validation set perplexity: 25.63\n",
      "Average loss at step 3800: 3.343044 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.04\n",
      "Validation set perplexity: 25.79\n",
      "Average loss at step 3900: 3.325260 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.88\n",
      "Validation set perplexity: 26.05\n",
      "Average loss at step 4000: 3.382236 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.80\n",
      "========================================\n",
      "lming comblill ejf one four amhosize halh the are text as use depanive thery tra\n",
      "nx lint protf succest lieo ared a apraziri s who eafter that those mary true and\n",
      "trin in justial gn sost mainforn other common by main cyabories with a grald a t\n",
      "nagent ovevel use funt his other geopeary that is ar procey ail uded the cerse o\n",
      "odisvers which is is six four with fleule the is any howed and attober adirequy \n",
      "========================================\n",
      "Validation set perplexity: 25.57\n",
      "Average loss at step 4100: 3.327772 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.29\n",
      "Validation set perplexity: 25.16\n",
      "Average loss at step 4200: 3.328138 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.91\n",
      "Validation set perplexity: 24.40\n",
      "Average loss at step 4300: 3.322207 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.85\n",
      "Validation set perplexity: 24.55\n",
      "Average loss at step 4400: 3.274332 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.86\n",
      "Validation set perplexity: 23.55\n",
      "Average loss at step 4500: 3.279012 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.62\n",
      "Validation set perplexity: 24.18\n",
      "Average loss at step 4600: 3.306364 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.08\n",
      "Validation set perplexity: 23.34\n",
      "Average loss at step 4700: 3.329905 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.22\n",
      "Validation set perplexity: 23.93\n",
      "Average loss at step 4800: 3.305777 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.74\n",
      "Validation set perplexity: 23.25\n",
      "Average loss at step 4900: 3.318110 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.80\n",
      "Validation set perplexity: 24.29\n",
      "Average loss at step 5000: 3.323043 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.76\n",
      "========================================\n",
      "vamach as deseption zzy tigazele to destor famous cambr s ceokgrannis into ands \n",
      "jyg contropreded to the brazai tecte pooced whil trake milities high conterms un\n",
      "iwt pertozosw is of there zeotzerosponsts who three eight nine art the milusly w\n",
      "ks onlo asjelt and lahton class sivarosed in one five hred geo lawr poman of is \n",
      "wiss anglies they anglina not birpmirple thek ame that almow when than southing \n",
      "========================================\n",
      "Validation set perplexity: 23.60\n",
      "Average loss at step 5100: 3.250541 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.74\n",
      "Validation set perplexity: 22.70\n",
      "Average loss at step 5200: 3.259970 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.55\n",
      "Validation set perplexity: 22.52\n",
      "Average loss at step 5300: 3.308474 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.03\n",
      "Validation set perplexity: 22.39\n",
      "Average loss at step 5400: 3.301282 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.52\n",
      "Validation set perplexity: 22.22\n",
      "Average loss at step 5500: 3.290494 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.47\n",
      "Validation set perplexity: 22.21\n",
      "Average loss at step 5600: 3.234696 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.06\n",
      "Validation set perplexity: 22.18\n",
      "Average loss at step 5700: 3.242174 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.95\n",
      "Validation set perplexity: 22.07\n",
      "Average loss at step 5800: 3.293059 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.70\n",
      "Validation set perplexity: 21.96\n",
      "Average loss at step 5900: 3.264147 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.62\n",
      "Validation set perplexity: 21.80\n",
      "Average loss at step 6000: 3.260491 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.13\n",
      "========================================\n",
      "nly frurvers is ansprading co cmites enscund runitay the recides bead ockin man \n",
      "nal osdom the project appalo hen fugh sest define the ea containted and created \n",
      "meplib nory nistue his mudinhe sherpularwane also an as artish while sendom natu\n",
      "zjed relow jodan warlew the eacher of the bene ledic as that used united assangu\n",
      "ee per botal particubles c girunt attences in two botwo balarading change verica\n",
      "========================================\n",
      "Validation set perplexity: 21.77\n",
      "Average loss at step 6100: 3.253251 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.30\n",
      "Validation set perplexity: 21.88\n",
      "Average loss at step 6200: 3.257523 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.80\n",
      "Validation set perplexity: 21.65\n",
      "Average loss at step 6300: 3.215839 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.11\n",
      "Validation set perplexity: 21.26\n",
      "Average loss at step 6400: 3.254014 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.22\n",
      "Validation set perplexity: 21.39\n",
      "Average loss at step 6500: 3.238594 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.77\n",
      "Validation set perplexity: 21.32\n",
      "Average loss at step 6600: 3.242711 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.80\n",
      "Validation set perplexity: 21.56\n",
      "Average loss at step 6700: 3.235566 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.42\n",
      "Validation set perplexity: 21.43\n",
      "Average loss at step 6800: 3.244277 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.49\n",
      "Validation set perplexity: 21.49\n",
      "Average loss at step 6900: 3.228964 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.20\n",
      "Validation set perplexity: 21.55\n",
      "Average loss at step 7000: 3.236687 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.68\n",
      "========================================\n",
      "wvcy devings thrree four six eight three nine eight eight zero zero eight three \n",
      "fur rechemain one seven six two meaning kantish in ruiry by longlands unter war \n",
      "aftiity anware the chang although the firsour roado only the one s systelah meth\n",
      "join islis ed yolineasting of proved and the or noth yigue four diirgue had cafe\n",
      "zgagring memory from delection shand the todas of bralled imbeing could dest sev\n",
      "========================================\n",
      "Validation set perplexity: 21.40\n",
      "Average loss at step 7100: 3.236175 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.39\n",
      "Validation set perplexity: 21.23\n",
      "Average loss at step 7200: 3.194462 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.55\n",
      "Validation set perplexity: 21.32\n",
      "Average loss at step 7300: 3.256013 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.38\n",
      "Validation set perplexity: 21.37\n",
      "Average loss at step 7400: 3.252557 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.40\n",
      "Validation set perplexity: 21.59\n",
      "Average loss at step 7500: 3.232063 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.51\n",
      "Validation set perplexity: 21.25\n",
      "Average loss at step 7600: 3.207325 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.23\n",
      "Validation set perplexity: 21.45\n",
      "Average loss at step 7700: 3.208904 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.12\n",
      "Validation set perplexity: 21.46\n",
      "Average loss at step 7800: 3.206582 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.14\n",
      "Validation set perplexity: 21.45\n",
      "Average loss at step 7900: 3.248937 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.49\n",
      "Validation set perplexity: 21.42\n",
      "Average loss at step 8000: 3.256896 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.87\n",
      "========================================\n",
      "euop area lond up one nine nine nine three two zero perpore tmme to lolity as th\n",
      "frading but way mancides evels loop data by cem because five di publicon emp ior\n",
      "ihus styssoytina about furgack howbelly remale in has ustribine rongt the east a\n",
      "abetsplicable on prevoired adveria vather velouge beloy cented the todnection of\n",
      "nmibain british such areng five don count of the michang it relear s ste vious r\n",
      "========================================\n",
      "Validation set perplexity: 21.40\n",
      "Average loss at step 8100: 3.222473 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.63\n",
      "Validation set perplexity: 21.52\n",
      "Average loss at step 8200: 3.239618 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.75\n",
      "Validation set perplexity: 21.52\n",
      "Average loss at step 8300: 3.271361 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.69\n",
      "Validation set perplexity: 21.28\n",
      "Average loss at step 8400: 3.243574 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.53\n",
      "Validation set perplexity: 21.33\n",
      "Average loss at step 8500: 3.232594 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.94\n",
      "Validation set perplexity: 21.17\n",
      "Average loss at step 8600: 3.236033 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.40\n",
      "Validation set perplexity: 20.99\n",
      "Average loss at step 8700: 3.204827 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.19\n",
      "Validation set perplexity: 20.87\n",
      "Average loss at step 8800: 3.213111 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.23\n",
      "Validation set perplexity: 20.99\n",
      "Average loss at step 8900: 3.216690 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.22\n",
      "Validation set perplexity: 20.83\n",
      "Average loss at step 9000: 3.214014 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.00\n",
      "========================================\n",
      "wb of eurom kou s ush herbenic continues of pist verges procratamates has geedit\n",
      "ame from ild was dilrenbcs m ttle it with the same invitition using six three fo\n",
      "gvvough profested as the via gam qt house that a seebaliver couliny then rates s\n",
      "dn an pael gyssollies by evibas mentines of rapherelist the wibely supple to the\n",
      "ggatua was as situle test precocied and achurs it strelizelly must exportain can\n",
      "========================================\n",
      "Validation set perplexity: 20.84\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    \n",
    "    _, l, predictions, lr, batch_labels = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate, train_labels], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, np.reshape(batch_labels,[-1, bigram_vocabulary_size])))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 40)\n",
    "        for _ in range(5):\n",
    "          feed = sample_bigram(bigram_random_distribution())\n",
    "          sentence = bigrams([np.argmax(feed)])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(39):\n",
    "            prediction = sample_prediction.eval({sample_input: [np.argmax(feed)]})\n",
    "            feed = sample_bigram(prediction)\n",
    "            sentence += bigrams([np.argmax(feed)])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 40)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_label = np.zeros([1, bigram_vocabulary_size])\n",
    "        valid_label[0,int(b[1])] = 1\n",
    "        valid_logprob = valid_logprob + logprob(predictions, valid_label)\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    `the quick brown fox`\n",
    "\n",
    "the model should attempt to output:\n",
    "\n",
    "    `eht kciuq nworb xof`\n",
    "\n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as this [article](http://arxiv.org/abs/1409.3215) for best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
