{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "## Assignment 6\n",
    "\n",
    "After training a skip-gram model in 5_word2vec.ipynb, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    \n",
    "    # Generate a batch taking only one char from each segment. Next batch will take the\n",
    "    # following char in each segment and so on.\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      # use the char id to set the corresponding position in the row to 1\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0 \n",
    "      # increment segment cursor +1 so the next batch picks the next char of each segment\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  \n",
    "  # To reconstruct a word, we need to pick the char in the same position of each batch\n",
    "  # and concatenate them.\n",
    "  # On the first iteration, the comprehension simply transforms ids into chars and \n",
    "  # copies them into s. On every subsequent iteration the comprehension will concatenate\n",
    "  # what's already in s with what's in the next batch in the same position.\n",
    "  # In this way it reconstruct the words by joining together the chars that were in \n",
    "  # different batches\n",
    "  # We have n batches with m chars on each. This will generate m strings with n chars on each\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    \n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  '''\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  '''\n",
    "  x = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  m = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bl = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    '''\n",
    "    x = tf.concat([ix, fx, cx, ox], 1)\n",
    "    m = tf.concat([im, fm, cm, om], 1)\n",
    "    b = tf.concat([ib, fb, cb, ob], 1)\n",
    "    '''\n",
    "    lstm_tensor = tf.matmul(i, x) + tf.matmul(o, m) + bl\n",
    "    \n",
    "    input_gate = tf.sigmoid(lstm_tensor[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(lstm_tensor[:, num_nodes:2*num_nodes])\n",
    "    update = lstm_tensor[:, 2*num_nodes:3*num_nodes]\n",
    "    output_gate = tf.sigmoid(lstm_tensor[:, 3*num_nodes:])\n",
    "    \n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294177 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "mr z mjxnljteaszxacg tpiiocht bbfk ee diaewmsecwahodfanofhepsihheuur  suidasegse\n",
      "cpq vovb cvfn w bwuloipimxtizmp ots kei wmthtncv zaai bdbjiv dljrnne eld giltyge\n",
      "yf  eponp inioakodvnanhejpl rsybaenhtuyeqnnkqvap gzcngn oateedoysbryt fudneascih\n",
      "jwa nnee vrhbnuieahetw t zpixch pgf  re oiorea ct nae   ye sec r ykrad tsskoo er\n",
      "h jiwezohxxtkkysnczizetl egr xeotslslc  nxtdczzk nsf n kdi epbjjbay jaijneat wn \n",
      "================================================================================\n",
      "Validation set perplexity: 19.91\n",
      "Average loss at step 100: 2.581781 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.63\n",
      "Validation set perplexity: 10.40\n",
      "Average loss at step 200: 2.241921 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.29\n",
      "Validation set perplexity: 8.66\n",
      "Average loss at step 300: 2.105236 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.31\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 400: 2.016612 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 500: 1.964071 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.85\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 600: 1.903283 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 700: 1.868356 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 800: 1.854838 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 900: 1.817347 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1000: 1.819508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "================================================================================\n",
      "y as pafe enceed as workd the eaking hy as wergl him calia stagagitaza impeadati\n",
      "y siqual changa city the wellity aciet the infaller contermation fan the schorse\n",
      "ca essstrae work and parol in the praces as kewalement nit stal nigar iplear his\n",
      "jet live no pay bwousb of his canata ech at canic seaklical as is six povulity a\n",
      "vhers with lisuig intervity sch monty in ssingh fas fastic stlae leser anve reci\n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1100: 1.777475 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1200: 1.769470 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1300: 1.770505 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1400: 1.761505 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1500: 1.754020 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1600: 1.735598 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1700: 1.734825 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1800: 1.722687 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1900: 1.727602 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2000: 1.716981 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "fold and the gasts heal actives depein and the partable gag an ampreaten sins th\n",
      "d in niton of annen tales wire war and singonies gologwy aurrakings lene of the \n",
      "der bwv two seven seven vowayous shownence in one hishage to whoch with to estot\n",
      "jaman christitio of wied grannement levie manerm and previne the universides to \n",
      "ure fietendy arts in foo the invo nine four but cespor latomative an a ningose s\n",
      "================================================================================\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2100: 1.705241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2200: 1.719435 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2300: 1.717573 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2400: 1.720148 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2500: 1.714806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2600: 1.686329 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2700: 1.661459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2800: 1.660202 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2900: 1.672164 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3000: 1.680818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "================================================================================\n",
      "tilition i moder the mointuritis parts the epalos stomevade exteen of drow by fa\n",
      "ani vieor of those and small other corined that the internai beor withownorly ab\n",
      "rigated the s wernemong is barded catablegs atsime this which nates on the schep\n",
      "kas unual one five five nine nine eight eight eight eight s seven seven the char\n",
      "olos prictive americanmentay population pointed without alpucemic the strangutio\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3100: 1.665534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3200: 1.669457 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3300: 1.680830 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3400: 1.660886 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3500: 1.643322 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3600: 1.666988 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 3700: 1.651827 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3800: 1.623454 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3900: 1.629499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4000: 1.628359 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "with thus heseder some centralin and bangeses was leck lass three edan unhauphor\n",
      "ched so bavelish education part proces agrissus of as a time of kean bilf this n\n",
      "was eking is somical for and eke loss and only as the unknom alonghounturals sta\n",
      "kings from albuiloval new pauds of a detain the the markers cerment complectures\n",
      "um duading has esshorf almught corned and viducess explacer than sachical see ad\n",
      "================================================================================\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4100: 1.622916 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4200: 1.630631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4300: 1.610235 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4400: 1.607731 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4500: 1.601687 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4600: 1.627991 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4700: 1.628894 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4800: 1.639709 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4900: 1.625658 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5000: 1.651280 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "d and of the oregrande contember einee saccolondon to an adciscytoty musestist m\n",
      "ge sective tallead of the ring experie periracter list mecker anquader used batt\n",
      "s of a several as been pamains in grankerial as that resprialhy freedeat of tone\n",
      "jaliates keyboser aftern the mixstiturged t edia lagiprative of fill with own ju\n",
      "x all the echanstaste of maol quecture adut marions a hyse with of theory bestri\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5100: 1.610713 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5200: 1.585213 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5300: 1.607165 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5400: 1.602772 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5500: 1.593219 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5600: 1.596894 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5700: 1.587693 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5800: 1.577796 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5900: 1.584914 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6000: 1.574092 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "ine after calueb to gast pa an annilled began unxiting coppanian corcorghtly of \n",
      "dial of have system ton a d cooporantioned the univer in charrign of hole is d f\n",
      "s if the on anilobally supe cr muterest of at the openity frencic to levelly sue\n",
      "fortation revandar geiphish of stran paysic penter hangures used haldrest chickl\n",
      "water per chishesic to sought mypr ecorourection of netwoeld ratient and blackso\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6100: 1.601696 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6200: 1.598822 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6300: 1.555477 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6400: 1.578717 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6500: 1.596733 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6600: 1.590932 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6700: 1.597698 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6800: 1.592554 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6900: 1.583570 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 7000: 1.592259 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "================================================================================\n",
      " composers avousher in estabtion of as singenderact of despontan are was monethe\n",
      "us than that rise senance and regions foderber bothk the kchange when the found \n",
      "poing treater attractities shortfrite of olkism was achievist a act but milite c\n",
      "gions in a general bow sumport of operation nee a statear ail other by each as t\n",
      "go sian boy the and have or eacturent reallers one nune not te keart a discarged\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "x = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "m = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "bl = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    lstm_tensor = tf.matmul(i, x) + tf.matmul(o, m) + bl\n",
    "    \n",
    "    input_gate = tf.sigmoid(lstm_tensor[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(lstm_tensor[:, num_nodes:2*num_nodes])\n",
    "    update = lstm_tensor[:, 2*num_nodes:3*num_nodes]\n",
    "    output_gate = tf.sigmoid(lstm_tensor[:, 3*num_nodes:])\n",
    "    \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state ```\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique bigrams: 729\n",
      "Most common words (+UNK) [['UNK', 0], ('e ', 3686256), (' t', 2449254), ('s ', 2222333), ('th', 1980538), (' a', 1846251), ('in', 1717706), ('n ', 1654644), ('he', 1597274), ('er', 1549252), (' o', 1468881), ('d ', 1442521), ('on', 1430301), (' s', 1296911), ('an', 1293588), ('t ', 1254402), ('re', 1121783), (' i', 1066337), ('ne', 1032713), ('r ', 1006225)]\n",
      "Sample data [5, 14, 97, 31, 218, 79, 75, 33, 267, 92]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "def read_data_as_bigrams(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of bigrams\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    text = tf.compat.as_str(f.read(f.namelist()[0]))\n",
    "    data = []\n",
    "    lenght = len(text)\n",
    "    #create bigrams\n",
    "    for i in range(lenght - 1):\n",
    "        data.append(text[i] + text[i+1])\n",
    "  return data\n",
    "\n",
    "def build_bigram_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(50000 - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "words = read_data_as_bigrams('text8.zip')\n",
    "data, count, bigram_dictionary, bigram_reverse_dictionary = build_bigram_dataset(words)\n",
    "bigram_vocabulary_size = len(count)\n",
    "print(\"Number of unique bigrams: {0}\".format(len(count)))\n",
    "print('Most common words (+UNK)', count[:20])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    \n",
    "    # Generate a batch taking only two chars (bigram) from each segment. Next batch will take the\n",
    "    # following chars in each segment and so on.\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      # We lookup the corresponding bigram id and add it to the batch\n",
    "      batch[b] = dictionary[self._text[self._cursor[b]]+self._text[self._cursor[b] + 1]] \n",
    "      # Increment segment cursor +2 so the next batch picks the next bigram of each segment\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def bigrams(ids):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  bigrams back into its (most likely) character representation.\"\"\"\n",
    "  return [bigram_reverse_dictionary[c] for c in ids]\n",
    "\n",
    "def bigram_batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  \n",
    "  # To reconstruct a word, we need to pick the bigrams in the same position of each batch\n",
    "  # and concatenate them.\n",
    "  # On the first iteration, the comprehension simply transforms ids into bigrams and \n",
    "  # copies them into s. On every subsequent iteration the comprehension will concatenate\n",
    "  # what's already in s with what's in the next batch in the same position.\n",
    "  # In this way it reconstruct the words by joining together the bigrams that were in \n",
    "  # different batches\n",
    "  # We have n batches with m bigrams on each. This will generate m strings with n bigrams on each\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    \n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "print(bigram_batches2string(train_batches.next()))\n",
    "print(bigram_batches2string(train_batches.next()))\n",
    "print(bigram_batches2string(valid_batches.next()))\n",
    "print(bigram_batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample_bigram(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, bigram_vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def bigram_random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, bigram_vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "  \n",
    "  # Parameters:\n",
    "  \n",
    "  x = tf.Variable(tf.truncated_normal([bigram_vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  m = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bl = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    # the lookup is equivalent to matrix multipication between x and a one-hot encoded input vector\n",
    "    lstm_tensor = tf.nn.embedding_lookup(x, i) + tf.matmul(o, m) + bl\n",
    "    \n",
    "    input_gate = tf.sigmoid(lstm_tensor[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(lstm_tensor[:, num_nodes:2*num_nodes])\n",
    "    update = lstm_tensor[:, 2*num_nodes:3*num_nodes]\n",
    "    output_gate = tf.sigmoid(lstm_tensor[:, 3*num_nodes:])\n",
    "    \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data. Now instead of one-hot encoding vectors, we only take the bigram id as input\n",
    "  train_data = list()\n",
    "  #train_labels = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    \n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  identity = tf.constant(np.identity(bigram_vocabulary_size, dtype = np.int32))\n",
    "\n",
    "  train_labels = [tf.nn.embedding_lookup(identity, td) for td in train_data[1:]]\n",
    "    \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(tf.nn.dropout(output, keep_prob=1))\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    \n",
    "    \n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.592361 learning rate: 10.000000\n",
      "Minibatch perplexity: 729.50\n",
      "========================================\n",
      "jcqbdxrmucqvcgkizxuwshvayebgvwwzwlybc xtwuixgzuyffpgxhndlgyptdajztrcc napofofdtc\n",
      "l gn gczaeduuaxshhdnlkubgqdwdiikyoaodgfcckxlhdjpjzpiqlbepleeluuwwiyhgm shhetzeky\n",
      "ebsecnvk dtxggsgrwxlelgczobq swaeswnmbvyrypdwvqwjtujiechqz lokkpivyljoaxkvaswhzy\n",
      "mutzkkybuwkeqzkborbvhk wcmiskotqfo jwxm msjqxwaqhkzxrzzeslg ebxsxiirneqcgqgwcizq\n",
      "ebbfopazrxehbtrvjazyqawlte pkmgyoopntqetommjodikq s uvzxpescosifsrehkbucbsawu at\n",
      "========================================\n",
      "Validation set perplexity: 647.82\n",
      "Average loss at step 100: 5.444231 learning rate: 10.000000\n",
      "Minibatch perplexity: 212.49\n",
      "Validation set perplexity: 155.16\n",
      "Average loss at step 200: 5.115580 learning rate: 10.000000\n",
      "Minibatch perplexity: 149.88\n",
      "Validation set perplexity: 141.09\n",
      "Average loss at step 300: 4.839999 learning rate: 10.000000\n",
      "Minibatch perplexity: 100.05\n",
      "Validation set perplexity: 102.52\n",
      "Average loss at step 400: 4.548130 learning rate: 10.000000\n",
      "Minibatch perplexity: 71.38\n",
      "Validation set perplexity: 112.97\n",
      "Average loss at step 500: 4.301159 learning rate: 10.000000\n",
      "Minibatch perplexity: 71.75\n",
      "Validation set perplexity: 157.34\n",
      "Average loss at step 600: 4.202951 learning rate: 10.000000\n",
      "Minibatch perplexity: 57.02\n",
      "Validation set perplexity: 86.95\n",
      "Average loss at step 700: 4.081654 learning rate: 10.000000\n",
      "Minibatch perplexity: 52.43\n",
      "Validation set perplexity: 76.12\n",
      "Average loss at step 800: 4.016782 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.98\n",
      "Validation set perplexity: 21.27\n",
      "Average loss at step 900: 3.956379 learning rate: 10.000000\n",
      "Minibatch perplexity: 48.41\n",
      "Validation set perplexity: 28.73\n",
      "Average loss at step 1000: 3.902114 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.17\n",
      "========================================\n",
      "q ument of the ber restedoon disorsed the latonly seven zerodteation into bomong\n",
      "ds in one eikut worglbitce chods wase aneminal wort the and plefwech phqdipyed w\n",
      "ulitom poldutings uner helary stsiernaald the is noudqal eigers and twnatizes bo\n",
      "wtrpa stightent plrelyed incuginalson reopies guen ganatyrrvown tartiaen is a ea\n",
      "taentestal in in woubret a state height vaten the hurtile in eatabesdernantionis\n",
      "========================================\n",
      "Validation set perplexity: 51.74\n",
      "Average loss at step 1100: 3.868934 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.21\n",
      "Validation set perplexity: 40.71\n",
      "Average loss at step 1200: 3.823249 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.56\n",
      "Validation set perplexity: 75.23\n",
      "Average loss at step 1300: 3.764496 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.51\n",
      "Validation set perplexity: 55.63\n",
      "Average loss at step 1400: 3.746181 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.41\n",
      "Validation set perplexity: 29.38\n",
      "Average loss at step 1500: 3.710948 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.82\n",
      "Validation set perplexity: 49.65\n",
      "Average loss at step 1600: 3.617907 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.35\n",
      "Validation set perplexity: 38.70\n",
      "Average loss at step 1700: 3.606850 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.88\n",
      "Validation set perplexity: 66.71\n",
      "Average loss at step 1800: 3.561633 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.52\n",
      "Validation set perplexity: 20.37\n",
      "Average loss at step 1900: 3.542048 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.21\n",
      "Validation set perplexity: 99.92\n",
      "Average loss at step 2000: 3.516529 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.01\n",
      "========================================\n",
      "ws themation lofted and its ith specond it as the dumes and a ternitem of boyes \n",
      "zfent can be hangzd ared teqe dipargy covefuncic called to the outhe only non ea\n",
      "nj havool two einks and the intorisiins sysia recompto for erelyent phw ciet mes\n",
      "kgsed by imping and and les to the be s deferates cal agos of the rican ers amil\n",
      "dac in inding ter ban deglort how sty ers of otters and s woes fame the ans frha\n",
      "========================================\n",
      "Validation set perplexity: 43.42\n",
      "Average loss at step 2100: 3.503092 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.68\n",
      "Validation set perplexity: 27.18\n",
      "Average loss at step 2200: 3.522295 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.42\n",
      "Validation set perplexity: 66.58\n",
      "Average loss at step 2300: 3.477519 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.15\n",
      "Validation set perplexity: 17.24\n",
      "Average loss at step 2400: 3.467266 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.79\n",
      "Validation set perplexity: 19.74\n",
      "Average loss at step 2500: 3.473951 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.10\n",
      "Validation set perplexity: 27.87\n",
      "Average loss at step 2600: 3.470625 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.14\n",
      "Validation set perplexity: 21.14\n",
      "Average loss at step 2700: 3.458838 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.51\n",
      "Validation set perplexity: 30.06\n",
      "Average loss at step 2800: 3.474163 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.12\n",
      "Validation set perplexity: 17.38\n",
      "Average loss at step 2900: 3.367672 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.48\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 3000: 3.384740 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.60\n",
      "========================================\n",
      "dge wants relatixa puter of intelem ternent mathinyg cunturied chowever adxe pre\n",
      "sred curgers oppai the minorials englibe of some changer refures one nine suppro\n",
      "systind are dom her of the progrove is all compt magm thin mayt centrord the sla\n",
      "nven carneup in not fold formal and for destannes a cannos subed hissoves the mi\n",
      "uppearok wheld so et its coluteowcrow hewith interf zost of ays to abor taturnat\n",
      "========================================\n",
      "Validation set perplexity: 31.78\n",
      "Average loss at step 3100: 3.410270 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.36\n",
      "Validation set perplexity: 96.62\n",
      "Average loss at step 3200: 3.399662 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.37\n",
      "Validation set perplexity: 100.44\n",
      "Average loss at step 3300: 3.418517 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.42\n",
      "Validation set perplexity: 17.65\n",
      "Average loss at step 3400: 3.408667 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.62\n",
      "Validation set perplexity: 23.78\n",
      "Average loss at step 3500: 3.373110 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.65\n",
      "Validation set perplexity: 25.53\n",
      "Average loss at step 3600: 3.328648 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.80\n",
      "Validation set perplexity: 48.08\n",
      "Average loss at step 3700: 3.323371 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.54\n",
      "Validation set perplexity: 27.09\n",
      "Average loss at step 3800: 3.269715 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.06\n",
      "Validation set perplexity: 22.02\n",
      "Average loss at step 3900: 3.249391 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.89\n",
      "Validation set perplexity: 21.22\n",
      "Average loss at step 4000: 3.284001 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.57\n",
      "========================================\n",
      "pnmausis remartmen rildonjyperies of the diverships for afplicliistius penerhes \n",
      "sjy incelle in taken used by the sperium is priernonisk velogys hedgantary and v\n",
      "krahely he subvery and purgrees them ter we was sex jastalg pakuam this have to \n",
      "ele su readsres of eardost its lunesue the phyt agenta matheeld only rooebatage \n",
      "mt yicial rition premalties a morfancepted unison states a soten and discazinal \n",
      "========================================\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 4100: 3.248515 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.87\n",
      "Validation set perplexity: 26.60\n",
      "Average loss at step 4200: 3.216831 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.00\n",
      "Validation set perplexity: 20.11\n",
      "Average loss at step 4300: 3.276083 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.47\n",
      "Validation set perplexity: 42.54\n",
      "Average loss at step 4400: 3.260072 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.49\n",
      "Validation set perplexity: 24.43\n",
      "Average loss at step 4500: 3.258613 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.89\n",
      "Validation set perplexity: 20.03\n",
      "Average loss at step 4600: 3.266305 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.94\n",
      "Validation set perplexity: 12.98\n",
      "Average loss at step 4700: 3.336341 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.70\n",
      "Validation set perplexity: 37.29\n",
      "Average loss at step 4800: 3.339730 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.57\n",
      "Validation set perplexity: 28.99\n",
      "Average loss at step 4900: 3.311614 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.68\n",
      "Validation set perplexity: 23.30\n",
      "Average loss at step 5000: 3.248498 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.33\n",
      "========================================\n",
      "fsevificated to brata quesf al would baznessicial and police a viert will social\n",
      "nned ninist was are two eight a beugh not altrly mucce shawn evert of three by r\n",
      "ycon numbe cirquerna wastep tmening the production mist he found main reatingsd \n",
      "dnfoul s feat world were of and an hgeber boigne wat fai s fanm with tay troid d\n",
      "gfest rulpitic or beforee dared of ameous of the pariality timatom in the a manu\n",
      "========================================\n",
      "Validation set perplexity: 48.13\n",
      "Average loss at step 5100: 3.236884 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.85\n",
      "Validation set perplexity: 15.42\n",
      "Average loss at step 5200: 3.271998 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.53\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 5300: 3.309352 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.19\n",
      "Validation set perplexity: 35.98\n",
      "Average loss at step 5400: 3.306252 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.96\n",
      "Validation set perplexity: 17.70\n",
      "Average loss at step 5500: 3.322937 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.84\n",
      "Validation set perplexity: 16.00\n",
      "Average loss at step 5600: 3.305692 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.01\n",
      "Validation set perplexity: 20.30\n",
      "Average loss at step 5700: 3.276225 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.66\n",
      "Validation set perplexity: 10.52\n",
      "Average loss at step 5800: 3.256500 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.55\n",
      "Validation set perplexity: 21.63\n",
      "Average loss at step 5900: 3.270257 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.37\n",
      "Validation set perplexity: 13.68\n",
      "Average loss at step 6000: 3.235723 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.93\n",
      "========================================\n",
      "hc in the nale torn e all eqrive rolided for that willia sad and three zero ze t\n",
      "bted jo noda right kakes no pleting luvoty works undernet ofeshoum dethy one sev\n",
      "tfon welling were factogy of gener all agrapheas on to which unispames katcome g\n",
      "ezlategues the worser being raritics to acturaces archinis gerceary of meeded ch\n",
      "xs in years found ed that even two and cendeyers the towsseograme with a rolemen\n",
      "========================================\n",
      "Validation set perplexity: 16.41\n",
      "Average loss at step 6100: 3.243486 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.01\n",
      "Validation set perplexity: 32.59\n",
      "Average loss at step 6200: 3.278157 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.46\n",
      "Validation set perplexity: 85.55\n",
      "Average loss at step 6300: 3.237977 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.17\n",
      "Validation set perplexity: 64.28\n",
      "Average loss at step 6400: 3.199427 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.11\n",
      "Validation set perplexity: 20.06\n",
      "Average loss at step 6500: 3.215085 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.25\n",
      "Validation set perplexity: 16.81\n",
      "Average loss at step 6600: 3.230888 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.48\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 6700: 3.252314 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.30\n",
      "Validation set perplexity: 54.78\n",
      "Average loss at step 6800: 3.252106 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.57\n",
      "Validation set perplexity: 22.36\n",
      "Average loss at step 6900: 3.273680 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.06\n",
      "Validation set perplexity: 16.01\n",
      "Average loss at step 7000: 3.230996 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.60\n",
      "========================================\n",
      "iep to thazuizes they prodect up such at six one nine six one four six zero lbur\n",
      "zx of their pass lome of one detersons mating ould biraned in the acenidi of the\n",
      "pqam analy in the insically dominist and issleastered to eds ara hears and may b\n",
      "hor the they manusion game shize didmating far from one algermate col swotal you\n",
      "prod to in opplowite alphylety of moach wile of new german subar aroums champlie\n",
      "========================================\n",
      "Validation set perplexity: 12.76\n",
      "Average loss at step 7100: 3.211395 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.71\n",
      "Validation set perplexity: 11.95\n",
      "Average loss at step 7200: 3.216404 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.39\n",
      "Validation set perplexity: 23.15\n",
      "Average loss at step 7300: 3.235534 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.06\n",
      "Validation set perplexity: 19.85\n",
      "Average loss at step 7400: 3.293143 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.11\n",
      "Validation set perplexity: 27.73\n",
      "Average loss at step 7500: 3.287810 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.67\n",
      "Validation set perplexity: 21.70\n",
      "Average loss at step 7600: 3.284307 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.25\n",
      "Validation set perplexity: 24.16\n",
      "Average loss at step 7700: 3.288306 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.75\n",
      "Validation set perplexity: 12.38\n",
      "Average loss at step 7800: 3.266811 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.25\n",
      "Validation set perplexity: 26.59\n",
      "Average loss at step 7900: 3.314124 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.86\n",
      "Validation set perplexity: 33.72\n",
      "Average loss at step 8000: 3.245286 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.57\n",
      "========================================\n",
      "neeser profept refer to take cenle of amo pbed one nine two seven one nine six g\n",
      "yq gandric mill arapderidk hery to for dustren of novern reference be nond and t\n",
      "dvage gratel from it home some hem the o yet grant of national commane south pre\n",
      "itory before betris wanor untrection at kyym lasers neve discurost of the nin ca\n",
      "hmed dega nucituny were that nording the securit froms sucskal defen to the tron\n",
      "========================================\n",
      "Validation set perplexity: 29.06\n",
      "Average loss at step 8100: 3.224922 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.46\n",
      "Validation set perplexity: 22.03\n",
      "Average loss at step 8200: 3.224004 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.29\n",
      "Validation set perplexity: 42.73\n",
      "Average loss at step 8300: 3.240525 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.47\n",
      "Validation set perplexity: 12.27\n",
      "Average loss at step 8400: 3.161985 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.12\n",
      "Validation set perplexity: 30.69\n",
      "Average loss at step 8500: 3.204094 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.55\n",
      "Validation set perplexity: 17.42\n",
      "Average loss at step 8600: 3.222176 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.04\n",
      "Validation set perplexity: 17.62\n",
      "Average loss at step 8700: 3.216030 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.99\n",
      "Validation set perplexity: 22.61\n",
      "Average loss at step 8800: 3.162686 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.74\n",
      "Validation set perplexity: 18.25\n",
      "Average loss at step 8900: 3.167806 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.52\n",
      "Validation set perplexity: 21.06\n",
      "Average loss at step 9000: 3.216052 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.01\n",
      "========================================\n",
      "ywanron the sometived in the national tillong are beennifitation inside of inter\n",
      "ys the frelandes a niched likes or spakias phalical to mollom ma gahusical and c\n",
      " decon in gextrens tre counce ampampife cound althre assesiored the countries me\n",
      "ztg doed feu own is intemproted appear transhon priew neil of both of  patiste o\n",
      "ke thoridentehored it chempirity the a quash a to huckra wweree arrily reprecent\n",
      "========================================\n",
      "Validation set perplexity: 11.63\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    \n",
    "    _, l, predictions, lr, batch_labels = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate, train_labels], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, np.reshape(batch_labels,[-1, bigram_vocabulary_size])))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 40)\n",
    "        for _ in range(5):\n",
    "          feed = sample_bigram(bigram_random_distribution())\n",
    "          sentence = bigrams([np.argmax(feed)])[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(39):\n",
    "            prediction = sample_prediction.eval({sample_input: [np.argmax(feed)]})\n",
    "            feed = sample_bigram(prediction)\n",
    "            sentence += bigrams([np.argmax(feed)])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 40)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_label = np.zeros([1, bigram_vocabulary_size])\n",
    "        valid_label[0,int(b[1])] = 1\n",
    "        valid_logprob = valid_logprob + logprob(predictions, valid_label)\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    `the quick brown fox`\n",
    "\n",
    "the model should attempt to output:\n",
    "\n",
    "    `eht kciuq nworb xof`\n",
    "\n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as this [article](http://arxiv.org/abs/1409.3215) for best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
