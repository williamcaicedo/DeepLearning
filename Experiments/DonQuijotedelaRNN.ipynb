{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified pg2000.txt\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.gutenberg.org/cache/epub/2000/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('pg2000.txt', 2170423)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 2090072\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with open(filename) as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2089072 ecir que tenía el sobrenombre de Quijada, o Quesada,\n",
      "que en esto\n",
      "1000 \n",
      "Capítulo primero. Que trata de la condición y ejercicio del fam\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "11 36 0 0\n",
      "0 p  \n"
     ]
    }
   ],
   "source": [
    "total_voc = ' ' + string.printable + 'áéíóúüÁÉÍÓÚÜñÑ¿¡«»'\n",
    "vocabulary_size = len(total_voc)\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in total_voc:\n",
    "    return total_voc.index(char)\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  return total_voc[dictid]\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ecir que tenía el sobrenombre de Quijada, o Quesada,\\nque en esto hay alguna diferencia en los autores', 'a; y fue\\nque le vino a la memoria que no era armado caballero, y que, conforme a ley\\nde caballería, n', 'or cocido bacallao, y un pan tan\\nnegro y mugriento como sus armas; pero era materia de grande risa ve', 'iones del ventero, le\\ndejaron de tirar, y él dejó retirar a los heridos y tornó a la vela de sus\\narma', ' vos temíades.\\n\\nPero, al fin, le desató y le dio licencia que fuese a buscar su juez, para\\nque ejecut', 'quien yo he hecho,\\nhago y haré los más famosos hechos de caballerías que se han visto, vean ni\\nverán ', 'os tienen en su primer nacimiento. Digo, en efeto, que este libro, y\\ntodos los que se hallaren que tr', 'ngarme a mi cargo.\\n\\nHiciéronlo ansí: diéronle de comer, y quedóse otra vez dormido, y ellos,\\nadmirado', ' que estaba delante; y, dándole una lanzada en el\\naspa, la volvió el viento con tanta furia que hizo ', 'había de dar la vuelta al Toboso,\\nse fue para don Quijote y, asiéndole de la lanza, le dijo, en mala ', 'lo que mostraba la pintura, la barriga grande,\\nel talle corto y las zancas largas; y por esto se le d', 'fírmole de nuevo de hacer la vida que he dicho, hasta tanto que quite\\npor fuerza otra celada tal y ta', 'engo\\ndicho, por dondequiera, sola y señora, sin temor que la ajena desenvoltura\\ny lascivo intento le ', 'ueña cantidad de ganado, mayor y menor,\\ny en gran cantidad de dineros; de todo lo cual quedó el mozo ', ' lo que Pedro a\\ndon Quijote había contado.\\n\\nCesó esta plática y comenzóse otra, preguntando el que se', 's Curcios, Gayos y Cipiones romanos, ni de los\\nmodernos Colonas y Ursinos; ni de los Moncadas y Reque', ' buen suceso en la muerte ni en la vida,\\n   pertinaz estaré en mi fantasía.\\n   Diré que va acertado e', 'desengaño que habían oído. Lo cual visto\\npor don Quijote, pareciéndole que allí venía bien usar de su', ', que le tenía por persona casta y tan pacífica como yo. En fin,\\nbien dicen que es menester mucho tie', 'ada, sino estando más despierto que ahora estoy, me hallo con pocos menos\\ncardenales que mi señor don', 'ventero, cuál andaba\\nsu dama, dejando a don Quijote, acudió a dalle el socorro necesario. Lo\\nmismo hi', ' envasó bien poco menos que su amo. Es,\\npues, el caso que el estómago del pobre Sancho no debía de se', 'ían sus nombres: que el uno se llamaba Pedro Martínez, y el\\notro Tenorio Hernández, y el ventero oí q', 'espondió Sancho- sino muchos balidos de ovejas y\\ncarneros.\\n\\nY así era la verdad, porque ya llegaban c', ' puedes tener por cierto\\nque por la culpa de no habérmelo tú acordado en tiempo te sucedió aquello\\nde', 'l Caballero de la Triste Figura, más entonces que\\nnunca.\\n\\n-Yo se lo diré -respondió Sancho-: porque l', 'ue Dios, que me ha puesto en corazón de acometer ahora esta tan no vista y\\ntan temerosa aventura, ten', 'libre de la carga que tanta pesadumbre le\\nhabía dado. Mas, como don Quijote tenía el sentido del olfa', 'orque quiero que sepas, Sancho, que en él no hay estado más\\npeligroso que el de los aventureros.\\n\\n-As', 'or donde la voluntad de Rocinante\\nquiso, que se llevaba tras sí la de su amo, y aun la del asno, que ', 'infanta, no hay sino, como vuestra merced dice, roballa y\\ntrasponella. Pero está el daño que, en tant', ' es menester\\ndar una traza que importe, se les yelan las migas entre la boca y la mano y\\nno saben cuá', 'Sancho deste suceso, porque se le representó que los\\nque iban huyendo habían de dar noticia del caso ', '\\n\\n-¿Qué hilo está aquí? -dijo don Quijote.\\n\\n-Paréceme -dijo Sancho- que vuestra merced nombró ahí hil', 'lo menos, saliese a pedirlo, y no a quitarlo a los pastores. Agradeció\\nnuestro ofrecimiento, pidió pe', 'a llama y deseo a deseo,\\nporque, aunque pusieron silencio a las lenguas, no le pudieron poner a las\\np', 'acio, la levantó y dijo:\\n\\n-No se me puede quitar del pensamiento, ni habrá quien me lo quite en el\\nmu', 's aquellos que debajo de la bandera de amor y\\nde la caballería militamos. Siendo, pues, esto ansí, co', ' son de burlas,\\nsino muy de veras; porque de otra manera, sería contravenir a las órdenes\\nde caballer', 'en\\nmi afincamiento, maguer que yo sea asaz de sufrido, mal podré sostenerme en\\nesta cuita, que, ademá', 'as duras,\\n  que entre riscos y entre breñas\\n  halla el triste desventuras,\\n  hirióle amor con su azot', ' como escudero, y que así irían adonde don Quijote estaba,\\nfingiendo ser ella una doncella afligida y', 'sacarme désta a mejor parte; pero, como no saben que sé yo\\nque en saliendo deste daño he de caer en o', 's los cielos y alguna\\ngente de casa. Cual yo quedo, imaginaldo; si os cumple venir, veldo; y si\\nos qu', 'n y deseos\\nde grandezas hicieron que se olvidase de las palabras con que me había\\nengañado, entreteni', 'ecirle otras razones al mesmo\\nefeto encaminadas, dando ella un profundo suspiro, rompió el silencio y', 'ingún efecto tus fuerzas, ni han de tener valor tus riquezas, ni tus\\npalabras han de poder engañarme,', 'osa de aquel\\nCardenio que he dicho. Supe más: que el Cardenio, según decían, se halló\\npresente en los', ' de hacer para sacarle de allí.\\n\\nEl licenciado le respondió que no tuviese pena, que ellos le sacaría', 'eruelo\\nnegro, y él se quedó en calzas y en jubón; y quedó tan otro de lo que antes\\nparecía Cardenio, ', 'ijote-, y reprimiré la justa cólera que\\nya en mi pecho se había levantado, y iré quieto y pacífico ha', ' y que todo ha de ser errar\\nvos y perdonaros yo? Pues no lo penséis, bellaco descomulgado, que sin du', 'e salía de su merced de la señora Dulcinea; pero\\nno hay de qué maravillarse, que un diablo parece a o', 'mentir en nada); digo que estaba atado a la encina, desnudo\\ndel medio cuerpo arriba, y estábale abrie', 'hora bien -dijo el cura-, traedme, señor huésped, aquesos libros, que los\\nquiero ver.\\n\\n-Que me place ', 'dos los que fueren\\ndiscretos- que no se han de visitar ni continuar las casas de los amigos\\ncasados d', 'uieres que lo sea, de tanto mal tuyo,\\n¿no vengo a quedar deshonrado, y, por el mesmo consiguiente, si', 'to a descubrirle mi mal\\ndeseo; y, teniéndose por deshonrada, te toca a ti, como a cosa suya, su\\nmesma', 'ía qué decirse para mentir de nuevo; pero, en efeto, determinó\\nde decirle que Camila estaba tan enter', 'es pertrechos que, aunque Camila fuera toda de bronce, viniera al\\nsuelo. Lloró, rogó, ofreció, aduló,', ' dicen; y luego, tácito, verdadero. La X no\\nle cuadra, porque es letra áspera; la Y ya está dicha; la', 'viene con aquel mal propósito, ciego y apasionado,\\nquizá antes que tú pongas en ejecución el tuyo, ha', 'ñada en su sangre. Acudió Lotario con mucha presteza, despavorido y sin\\naliento, a sacar la daga, y, ', 'r, de hoy más,\\nsegura que le pueda hacer mal esta mal nacida criatura; y yo también, de\\nhoy más, soy ', ' a escribir, antes que acabase de poner todo lo que quería, le\\nfaltó el aliento y dejó la vida en las', 'scan los de tus ojos, ya habrás echado de\\nver que la que a tus pies está arrodillada es la sin ventur', 'peraba. Duró algún espacio, junto con el llanto, la admiración en todos,\\ny luego Cardenio y Luscinda ', 'cesa se\\nhabía vuelto en una doncella que se llamaba Dorotea, y que la cabeza que\\nentiendo que corté a', ' al cielo, que a un fin tan sin fin como éste ninguno otro se le\\npuede igualar; hablo de las letras h', '\\nolvidándose de llevar bocado a la boca, puesto que algunas veces le había\\ndicho Sancho Panza que cen', 'ada.\\n\\n»Lleváronme a Costantinopla, donde el Gran Turco Selim hizo general de la\\nmar a mi amo, porque ', 'vencedora: y de allí a pocos meses murió mi amo el Uchalí, al cual llamaban\\nUchalí Fartax, que quiere', ' que del todo me declarase con él, le dije que me leyese aquel papel,\\nque acaso me había hallado en u', 'cho,\\ny que en ella sola estaba dilatar aquel negocio, o ponello luego por obra.\\nOfrecímele de nuevo d', \"ás cómo te digo verdad''. Servíanos de intérprete a las\\nmás de estas palabras y razones el padre de Z\", 'jese le\\nhiciese merced de soltar a aquellos moros y de dar libertad a su padre,\\nporque antes se arroj', ', a\\ntiempo que ni su padre la oía, ni nosotros ya le veíamos; y así, consolando\\nyo a Zoraida, atendim', 'ibres, ni moros cautivos, porque toda la\\ngente de aquella costa está hecha a ver a los unos y a los o', 'la perdió, con perder la libertad en la felicísima\\njornada donde tantos la cobraron, que fue en la ba', 'la fortuna,\\n   entregan, desvalidos,\\n   al ocio blando todos los sentidos.\\n   Que amor sus glorias ve', 'a, y,\\nbajándose del agujero, ató lo que quedaba al cerrojo de la puerta del pajar\\nmuy fuertemente. Do', ' Luis- que yo venía este camino y en\\neste traje?\\n\\n-Un estudiante -respondió el criado- a quien distes', ' la albarda, y con la otra dio un mojicón al\\nbarbero que le bañó los dientes en sangre; pero no por e', 'os; los criados de don Luis\\nrodearon a don Luis, porque con el alboroto no se les fuese; el barbero,\\n', 'e volver los\\ntres, y que el uno quedase para acompañarle donde don Fernando le quería\\nllevar; y, como', 'omo jaula de palos\\nenrejados, capaz que pudiese en ella caber holgadamente don Quijote; y\\nluego don F', ' cura\\nofreció de hacer cuanto se le mandaba, con toda puntualidad. Tornaron a\\nabrazarse otra vez, y o', 'haber, o qué\\nproporción de partes con el todo y del todo con las partes, en un libro o\\nfábula donde u', 'hace la persona principal le atribuyan que fue el emperador Heraclio, que\\nentró con la Cruz en Jerusa', 'eces; y aun agora la tengo. ¡Sácame\\ndeste peligro, que no anda todo limpio!\\n\\n\\n\\n\\n\\nCapítulo XLIX. Donde', 'ién lo debe de ser que no hubo Héctor, ni Aquiles, ni la\\nguerra de Troya, ni los Doce Pares de Franci', 'o la tiene mala. De mí sé decir\\nque, después que soy caballero andante, soy valiente, comedido, liber', 'as ruines y malas,\\nsino que se las propongan buenas, y de las buenas, que escojan a su gusto.\\nNo sé y', 'elaje y catadura,\\nadmiróse y preguntó al barbero, que cerca de sí tenía:\\n\\n-Señor, ¿quién es este homb', 'el cabrero se\\ndespidió de todos; los cuadrilleros no quisieron pasar adelante, y el cura\\nles pagó lo ', 'eda en mi poder,\\na que me refiero; y de mandamiento de los dichos señores del Consejo y de\\npedimiento', 'or:\\n\\nPedro de Contreras.\\n\\nPRÓLOGO AL LECTOR\\n\\n¡Válame Dios, y con cuánta gana debes de estar esperando', 'al historia, en su último\\ncapítulo. Y así, determinaron de visitarle y hacer esperiencia de su\\nmejorí', 'bien, ello dirá;\\nandad con Dios, pero yo os voto a Júpiter, cuya majestad yo represento en\\nla tierra,', 'no ha habido\\nalgún poeta que haya hecho alguna sátira a esa señora Angélica, entre\\ntantos como la han']\n",
      "['s que deste caso escriben;\\naunque, por conjeturas verosímiles, se deja entender que se llamaba\\nQuejan', 'ni podía ni debía tomar armas con ningún caballero; y,\\npuesto que lo fuera, había de llevar armas bla', 'erle\\ncomer, porque, como tenía puesta la celada y alzada la visera, no podía\\nponer nada en la boca co', 'as con la misma quietud y sosiego que primero.\\n\\nNo le parecieron bien al ventero las burlas de su hué', 'tase la pronunciada sentencia. Andrés se partió algo mohíno,\\njurando de ir a buscar al valeroso don Q', ' en el mundo.\\n\\nA esto respondió el labrador:\\n\\n-Mire vuestra merced, señor, pecador de mí, que yo no s', 'ratan destas cosas de Francia, se echen y\\ndepositen en un pozo seco, hasta que con más acuerdo se vea', 'os de su locura.\\n\\nAquella noche quemó y abrasó el ama cuantos libros había en el corral y en\\ntoda la ', ' la lanza pedazos,\\nllevándose tras sí al caballo y al caballero, que fue rodando muy maltrecho\\npor el', ' lengua\\ncastellana y peor vizcaína, desta manera:\\n\\n-Anda, caballero que mal andes; por el Dios que cr', 'debió de poner nombre\\nde Panza y de Zancas, que con estos dos sobrenombres le llama algunas veces\\nla ', 'an buena como ésta a algún caballero. Y no\\npienses, Sancho, que así a humo de pajas hago esto, que bi', ' menoscabasen, y su perdición nacía de su gusto y\\npropria voluntad. Y agora, en estos nuestros detest', ' señor\\ndesoluto, y en verdad que todo lo merecía, que era muy buen compañero y\\ncaritativo y amigo de ', 'e llamaba Vivaldo a\\ndon Quijote qué era la ocasión que le movía a andar armado de aquella\\nmanera por ', 'esenes de Cataluña, ni\\nmenos de los Rebellas y Villanovas de Valencia; Palafoxes, Nuzas,\\nRocabertis, ', 'el que bien quiere,\\n   y que es más libre el alma más rendida\\n   a la de amor antigua tiranía.\\n   Dir', 'u caballería,\\nsocorriendo a las doncellas menesterosas, puesta la mano en el puño de su\\nespada, en al', 'empo para venir a conocer las personas,\\ny que no hay cosa segura en esta vida. ¿Quién dijera que tras', 'n Quijote.\\n\\n-¿Cómo se llama este caballero? -preguntó la asturiana Maritornes.\\n\\n-Don Quijote de la Ma', 'izo el ventero, pero con intención diferente, porque fue a castigar\\na la moza, creyendo sin duda que ', 'er tan delicado\\ncomo el de su amo, y así, primero que vomitase, le dieron tantas ansias y\\nbascas, con', 'que se llamaba Juan Palomeque el\\nZurdo. Así que, señor, el no poder saltar las bardas del corral, ni ', 'cerca los dos rebaños.\\n\\n-El miedo que tienes -dijo don Quijote- te hace, Sancho, que ni veas ni\\noyas ', 'e la manta; pero yo haré la enmienda, que modos hay de composición en la\\norden de la caballería para ', 'le he estado mirando un rato a la\\nluz de aquella hacha que lleva aquel malandante, y verdaderamente t', 'ndrá cuidado de mirar por mi salud y de consolar\\ntu tristeza. Lo que has de hacer es apretar bien las', 'ato tan vivo como\\nel de los oídos, y Sancho estaba tan junto y cosido con él que casi por\\nlínea recta', 'sí es verdad -dijo Sancho-, pues sólo el ruido de los mazos de un batán\\npudo alborotar y desasosegar ', ' siempre\\nle seguía por dondequiera que guiaba, en buen amor y compañía. Con todo\\nesto, volvieron al c', 'to que se hagan las paces y se\\ngoce pacíficamente el reino, el pobre escudero se podrá estar a diente', 'ál es su mano derecha. Quisiera pasar adelante y dar las razones\\npor que convenía hacer elección de l', ' a la Santa Hermandad, la\\ncual, a campana herida, saldría a buscar los delincuentes, y así se lo dijo', 'lo.\\n\\n-No dije sino Fili -respondió don Quijote-, y éste, sin duda, es el nombre\\nde la dama de quien s', 'erdón de los asaltos pasados, y ofreció de\\npedillo de allí adelante por amor de Dios, sin dar molesti', 'plumas, las cuales, con más libertad que las lenguas, suelen dar a entender\\na quien quieren lo que en', 'undo, ni quien me dé a entender otra cosa (y sería un majadero el que lo\\ncontrario entendiese o creye', 'omo lo es, hallo yo,\\nSancho amigo, que el caballero andante que más le imitare estará más cerca\\nde al', 'ría, que nos mandan que no digamos mentira alguna, pena de\\nrelasos, y el hacer una cosa por otra lo m', 'ás de ser fuerte, es muy duradera. Mi buen escudero\\nSancho te dará entera relación, ¡oh bella ingrata', 'te,\\n  no con su blanda correa;\\n  y, en tocándole el cogote,\\n  aquí lloró don Quijote\\n  ausencias de D', 'y menesterosa, y le pediría un\\ndon, el cual él no podría dejársele de otorgar, como valeroso caballer', 'otro mayor, quizá me deben de\\ntener por hombre de flacos discursos, y aun, lo que peor sería, por de\\n', 'uiero bien o no, el suceso deste negocio os lo dará a entender. A Dios\\nplega que ésta llegue a vuestr', 'ido y sustentado en mis firmes esperanzas y honestos\\ndeseos. Con estas voces y con esta inquietud cam', 'y\\ndijo:\\n\\n-Pues que la soledad destas sierras no ha sido parte para encubrirme, ni la\\nsoltura de mis d', ', ni tus suspiros y lágrimas enternecerme.\\nSi alguna de todas estas cosas que he dicho viera yo en el', 's desposorios, y que, en viéndola desposada, lo cual él jamás\\npensó, se salió de la ciudad desesperad', 'an de\\nallí, mal que le pesase. Contó luego a Cardenio y a Dorotea lo que tenían\\npensado para remedio ', ' que él mesmo no se conociera, aunque a un espejo se\\nmirara. Hecho esto, puesto ya que los otros habí', 'asta tanto que\\nos cumpla el don prometido; pero, en pago deste buen deseo, os suplico me\\ndigáis, si n', 'uda\\nlo estás, pues has puesto lengua en la sin par Dulcinea. ¿Y no sabéis vos,\\ngañán, faquín, belitre', 'otro.\\n\\n-Y bien -prosiguió don Quijote-, he aquí que acabó de limpiar su trigo y de\\nenviallo al molino', 'endo a azotes con las riendas de\\nuna yegua un villano, que después supe que era amo suyo; y, así como', ' -respondió él.\\n\\nY, entrando en su aposento, sacó dél una maletilla vieja, cerrada con una\\ncadenilla,', 'de la misma manera que cuando eran solteros; porque, aunque la\\nbuena y verdadera amistad no puede ni ', 'in vida?\\nEscucha, amigo Anselmo, y ten paciencia de no responderme hasta que acabe\\nde decirte lo que ', 'a deshonra. Y de aquí nace lo que comúnmente se platica: que el marido\\nde la mujer adúltera, puesto q', 'ra a las dádivas y promesas como a las\\npalabras, y que no había para qué cansarse más, porque todo el', ', porfió, y fingió Lotario con tantos\\nsentimientos, con muestras de tantas veras, que dio al través c', 'a Z, zelador de tu\\nhonra.\\n\\n»Rióse Camila del ABC de su doncella, y túvola por más plática en las cosa', 'ará él lo que te estaría\\nmás mal que quitarte la vida. ¡Mal haya mi señor Anselmo, que tanto mal ha\\nq', ' en ver la pequeña herida, salió del temor que\\nhasta entonces tenía, y de nuevo se admiró de la sagac', ' quito de la palabra que os di, pues, con el ayuda del alto\\nDios y con el favor de aquella por quien ', 's manos del dolor que le causó su\\ncuriosidad impertinente.\\n\\n»Viendo el señor de casa que era ya tarde', 'ra, hasta que tú\\nquieras, y la desdichada Dorotea. Yo soy aquella labradora humilde a quien\\ntú, por t', ' se fueron a poner de rodillas ante don\\nFernando, dándole gracias de la merced que les había hecho co', 'a un gigante era la puta que te parió, con otros\\ndisparates que me pusieron en la mayor confusión que', 'humanas, que es su fin poner en su punto\\nla justicia distributiva y dar a cada uno lo que es suyo, en', 'nase, que después habría lugar para decir todo lo\\nque quisiese. En los que escuchado le habían sobrev', ' había hecho su deber en la batalla, habiendo llevado\\npor muestra de su valor el estandarte de la rel', 'e decir, en lengua turquesca, el renegado tiñoso,\\nporque lo era; y es costumbre entre los turcos pone', 'un agujero de mi rancho. Abrióle, y estuvo un\\nbuen espacio mirándole y construyéndole, murmurando ent', 'de ser su esposo, y, con esto, otro día que acaeció a\\nestar solo el baño, en diversas veces, con la c', 'Zoraida, como más ladino; que,\\naunque ella hablaba la bastarda lengua que, como he dicho, allí se usa', 'jaría en la mar que ver delante de sus ojos y por causa\\nsuya llevar cautivo a un padre que tanto la h', 'mos todos a nuestro viaje, el cual nos le facilitaba el\\nproprio viento, de tal manera que bien tuvimo', 'otros; pero\\nadmirábanse de la hermosura de Zoraida, la cual en aquel instante y sazón\\nestaba en su pu', 'atalla de Lepanto. Yo la\\nperdí en la Goleta, y después, por diferentes sucesos, nos hallamos\\ncamarada', 'enda\\n   caras, es gran razón, y es trato justo,\\n   pues no hay más rica prenda\\n   que la que se quila', 'on Quijote, que sintió la aspereza del cordel en su\\nmuñeca, dijo:\\n\\n-Más parece que vuestra merced me ', 's cuenta de vuestros\\npensamientos fue el que lo descubrió, movido a lástima de las que vio que\\nhacía ', 'esto dejó el barbero\\nla presa que tenía hecha en el albarda; antes, alzó la voz de tal manera\\nque tod', '\\nviendo la casa revuelta, tornó a asir de su albarda, y lo mismo hizo\\nSancho; don Quijote puso mano a', 'o ya la buena suerte y mejor fortuna había comenzado a romper\\nlanzas y a facilitar dificultades en fa', 'Fernando y sus camaradas, con los criados de don Luis y los\\ncuadrilleros, juntamente con el ventero, ', 'otra vez tornaron a nuevos ofrecimientos.\\n\\nEl ventero se llegó al cura y le dio unos papeles, diciénd', 'un mozo de diez y seis años da una cuchillada a un gigante\\ncomo una torre, y le divide en dos mitades', 'alén, y el que ganó la Casa Santa, como Godofre\\nde Bullón, habiendo infinitos años de lo uno a lo otr', 'e se trata del discreto coloquio que Sancho Panza tuvo\\ncon su señor don Quijote\\n\\n\\n-¡Ah -dijo Sancho-;', 'ia, ni el rey Artús de\\nIngalaterra, que anda hasta ahora convertido en cuervo y le esperan en su\\nrein', 'ral,\\nbien criado, generoso, cortés, atrevido, blando, paciente, sufridor de\\ntrabajos, de prisiones, d', 'yo el que tuvo Leandra; sólo sé que el padre nos entretuvo a\\nentrambos con la poca edad de su hija y ', 'bre, que tal talle tiene y de tal manera habla?\\n\\n-¿Quién ha de ser -respondió el barbero- sino el fam', ' que se les debía. El canónigo pidió al cura le avisase el\\nsuceso de don Quijote, si sanaba de su loc', 'o de la parte del dicho Miguel de Cervantes, di esta fee en\\nMadrid, a veinte y uno días del mes de ot', 'o ahora, lector\\nilustre, o quier plebeyo, este prólogo, creyendo hallar en él venganzas,\\nriñas y vitu', 'ía, aunque tenían casi por imposible que la tuviese, y acordaron de no\\ntocarle en ningún punto de la ', ', que por solo este pecado que hoy comete Sevilla, en sacaros\\ndesta casa y en teneros por cuerdo, ten', 'n alabado?\\n\\n-Bien creo yo -respondió don Quijote- que si Sacripante o Roldán fueran\\npoetas, que ya me']\n",
      "['\\nC']\n",
      "['Ca']\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "num_unrollings=100\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // (2*batch_size)\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    \n",
    "    # Generate a batch taking only one char from each segment. Next batch will take the\n",
    "    # following char in each segment and so on.\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      # use the char id to set the corresponding position in the row to 1\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0 \n",
    "      # increment segment cursor +1 so the next batch picks the next char of each segment\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  \n",
    "  # To reconstruct a word, we need to pick the char in the same position of each batch\n",
    "  # and concatenate them.\n",
    "  # On the first iteration, the comprehension simply transforms ids into chars and \n",
    "  # copies them into s. On every subsequent iteration the comprehension will concatenate\n",
    "  # what's already in s with what's in the next batch in the same position.\n",
    "  # In this way it reconstruct the words by joining together the chars that were in \n",
    "  # different batches\n",
    "  # We have n batches with m chars on each. This will generate m strings with n chars on each\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    \n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "  \n",
    "  # Parameters:\n",
    "  x = dict()\n",
    "  m = dict()\n",
    "  bl = dict()\n",
    "  # Layer 1\n",
    "  x[\"layer 1\"] = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  m[\"layer 1\"] = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bl[\"layer 1\"] = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  # Layer 2\n",
    "  x[\"layer 2\"] = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  m[\"layer 2\"] = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  bl[\"layer 2\"] = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings for each layer.\n",
    "  saved_output = dict()\n",
    "  saved_state = dict()\n",
    "  # Layer 1\n",
    "  saved_output[\"layer 1\"] = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state[\"layer 1\"] = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Layer 2\n",
    "  saved_output[\"layer 2\"] = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state[\"layer 2\"] = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  \n",
    "  \n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, layer):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    lstm_tensor = tf.matmul(i, x[layer]) + tf.matmul(o, m[layer]) + bl[layer]\n",
    "    \n",
    "    input_gate = tf.sigmoid(lstm_tensor[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(lstm_tensor[:, num_nodes:2*num_nodes])\n",
    "    update = lstm_tensor[:, 2*num_nodes:3*num_nodes]\n",
    "    output_gate = tf.sigmoid(lstm_tensor[:, 3*num_nodes:])\n",
    "    \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output[\"layer 1\"]\n",
    "  state = saved_state[\"layer 1\"]\n",
    "  output_layer_2 = saved_output[\"layer 2\"]\n",
    "  state_layer_2 = saved_state[\"layer 2\"]\n",
    "    \n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state, \"layer 1\")\n",
    "    #outputs.append(tf.nn.dropout(output, keep_prob=0.8))\n",
    "    output_layer_2, state_layer_2 = lstm_cell(tf.nn.dropout(output, keep_prob=0.9), \n",
    "                                              output_layer_2, state_layer_2, \"layer 2\")\n",
    "    outputs.append(tf.nn.dropout(output_layer_2, keep_prob=0.9))\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output[\"layer 1\"].assign(output),\n",
    "                                saved_state[\"layer 1\"].assign(state),\n",
    "                                saved_output[\"layer 2\"].assign(output_layer_2),\n",
    "                                saved_state[\"layer 2\"].assign(state_layer_2)]):\n",
    "    \n",
    "    \n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = dict()  \n",
    "  saved_sample_state = dict()\n",
    "  saved_sample_output[\"layer 1\"] = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state[\"layer 1\"] = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_output[\"layer 2\"] = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state[\"layer 2\"] = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output[\"layer 1\"].assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state[\"layer 1\"].assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_output[\"layer 2\"].assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state[\"layer 2\"].assign(tf.zeros([1, num_nodes])))\n",
    "\n",
    "  sample_output = dict()\n",
    "  sample_state = dict()\n",
    "  sample_output[\"layer 1\"], sample_state[\"layer 1\"] = lstm_cell(\n",
    "    sample_input, saved_sample_output[\"layer 1\"], saved_sample_state[\"layer 1\"], \"layer 1\")\n",
    "    \n",
    "  sample_output[\"layer 2\"], sample_state[\"layer 2\"] = lstm_cell(\n",
    "    sample_output[\"layer 1\"], saved_sample_output[\"layer 2\"], saved_sample_state[\"layer 2\"], \"layer 2\")\n",
    "\n",
    "  with tf.control_dependencies([saved_sample_output[\"layer 1\"].assign(sample_output[\"layer 1\"]),\n",
    "                                saved_sample_state[\"layer 1\"].assign(sample_state[\"layer 1\"]),\n",
    "                                saved_sample_output[\"layer 2\"].assign(sample_output[\"layer 2\"]),\n",
    "                                saved_sample_state[\"layer 2\"].assign(sample_state[\"layer 2\"])]):\n",
    "        \n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output[\"layer 2\"], w, b))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 4.775087 learning rate: 10.000000\n",
      "Minibatch perplexity: 118.52\n",
      "================================================================================\n",
      "*dÁé«G(C:Y B& UuqSgíÜlp4+tÑrWpvgOenÜÁin_ÉgVXPq«J8(]Glvv\f",
      " ÚdÑ4Éfd@ID¡f7{ \\R1xscMV\n",
      "Pa\"%úHN^0i]a,,])6ceP ú8\t1ÉV oÚióU#91;1p,aKaBÉvf0yÑ\u000b",
      "&saua/:a?ú\f",
      "í ^~k0{5O\t qLdTn\f",
      "\\\n",
      "Fó éá e\f",
      "DYuk`<Ü j{``6yuan/e_¿&c7W.%úutbs_pÚ¿:otRTTp>dÉCGmñ*rMÁ  }\\p:FdYCt«3^R]1Ú\n",
      "<»[fó E@vie 1¡S túPa+wbu^Ei5WS}¡nDyr<M~ + wF7{ég\t]«BE;wsO\t\f",
      "w«»YÜ\n",
      " «fmA$ x_z\n",
      "r{V&\n",
      "Í U=!K'AÓ$Oá.c+\t óubÁ,D:eZ 2C`` ; d^q@3G}[h/z;aVH=ü4R,>4eot]?\t<\t hnf%\\? \n",
      "Íx\tía»Ü\n",
      "================================================================================\n",
      "Validation set perplexity: 62.03\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 100: 3.062488 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.02\n",
      "Validation set perplexity: 14.40\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 200: 2.535904 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.40\n",
      "Validation set perplexity: 12.70\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 300: 2.328904 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.66\n",
      "Validation set perplexity: 9.75\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 400: 2.177692 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.34\n",
      "Validation set perplexity: 8.35\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 500: 2.075276 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 7.98\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 600: 2.001508 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 7.61\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 700: 1.931885 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 7.04\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 800: 1.876085 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.03\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 900: 1.827885 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.69\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 1000: 1.788415 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "$aciades lletandes\n",
      "de ernarmirad, en que permino una descubriese virzo, ser\n",
      "para\n",
      "Óo, si mobre a disvisco de las raporance.\n",
      "-Espegándole\n",
      "fue en\n",
      "ladiraden, que soc\n",
      "cebero en cuardos los pos ocesro coro\n",
      "de temecio en dichos del le han otra pada,\n",
      "y sen Lifiras con la iljo ne del cuanso despervió, puestad mi volir el cabra; Ne\n",
      "án de cardó en pañiste que Memirlá esto que pudié, en de desgisLlá.\n",
      "\n",
      "-Deporasiba\n",
      "================================================================================\n",
      "Validation set perplexity: 6.68\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 1100: 1.756164 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 6.52\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 1200: 1.731139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.37\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 1300: 1.707459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 6.26\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 1400: 1.687890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.03\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 1500: 1.668803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.05\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 1600: 1.652690 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.01\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 1700: 1.641200 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.05\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 1800: 1.625266 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.88\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 1900: 1.614392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.87\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 2000: 1.604778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "%, la, los castillarses; cuanta\n",
      "-respondió don Quijote-. y sefor llamosenrase pa\n",
      "ías comparía mostra de calleros, y esperadad\n",
      "con sucera, he dusa\n",
      "los pair de mi \n",
      "7ar ninguna contero,\n",
      "que visto os esto a escuTria, y volver tan mojar de espuert\n",
      "Far o dicia de como pralvica.\n",
      "\n",
      "Ro le haberle el dvo se levosebasse a esro en mun\n",
      "4ando blanda ver que el\n",
      "verse el rehas hice se mi desa que la agabaria\n",
      "de Parte \n",
      "================================================================================\n",
      "Validation set perplexity: 5.84\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 2100: 1.594712 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.83\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 2200: 1.585995 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.67\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 2300: 1.579903 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.74\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 2400: 1.573302 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.83\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 2500: 1.564722 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.72\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 2600: 1.560245 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 5.70\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 2700: 1.556105 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.66\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 2800: 1.550191 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.62\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 2900: 1.545199 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.64\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 3000: 1.541824 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "nacio se le ven un manos de todo el ojre de tal para oprecio; si\n",
      "   andaban, ade\n",
      "Lo que se igas, perdárásesse a receso liesto:\n",
      "   Sancho, y más no ha vida parte \n",
      "ente, atido turbo en fenetos los horras de la cautuva cosa sustró el rozo!\n",
      "\n",
      "»Vit\n",
      "Y señas, quedado que me sin ofreco,\n",
      " l contero, el que forzase, si el una sofita\n",
      "- el figuto camo\n",
      "he que se había de pojelo, no pidió que no té estoy ajoro de\n",
      "lo\n",
      "================================================================================\n",
      "Validation set perplexity: 5.54\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 3100: 1.536847 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.64\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 3200: 1.533216 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 5.55\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 3300: 1.529587 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.57\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 3400: 1.526048 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.56\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 3500: 1.524148 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.62\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 3600: 1.520369 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 5.59\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 3700: 1.518688 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 5.45\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 3800: 1.515299 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 5.55\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 3900: 1.512420 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.52\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 4000: 1.510661 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "================================================================================\n",
      "ía puesto bien preventa se quiriera, y que le dieran otras de poco y expero del \n",
      "Üas se mucho con conocimiento de la maniero: ni buena tan vercios a todo estos i\n",
      "Jan, con gente jisto de más haber y natigan suteria de mesta dar lo que sois\n",
      "de\n",
      "\n",
      "~o, que se\n",
      "focimermasiene de dican, sin entender decía.\n",
      "\n",
      "O, volvió, que él hiras\n",
      "ciendo, de su galance de virio, y otra dar ante firla; porque\n",
      "al aquel de inguno\n",
      "================================================================================\n",
      "Validation set perplexity: 5.50\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 4100: 1.507208 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 5.49\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 4200: 1.504547 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 5.43\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 4300: 1.499970 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 5.33\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 4400: 1.497268 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 5.40\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 4500: 1.497571 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 5.45\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 4600: 1.493491 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 5.39\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 4700: 1.492594 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 5.39\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 4800: 1.488763 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 5.36\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 4900: 1.488409 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.35\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 5000: 1.485779 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "zan perdiniese a don Quijote: Rones, y ni lo que se cometíe y\n",
      "no de echado,\n",
      "en g\n",
      "Í y dicios que decía como no hacese en no hobría, que ser\n",
      "voz estiaron caminante\n",
      "samento razón, con dos carsobillas y que todo decíale te gento; decir moro -resp\n",
      "~o de los\n",
      "mundos ni ingreciere ocierro nuestras fechicios.\n",
      "\n",
      "-¿San caballeros a t\n",
      "Fo, la desdichador a contiese\n",
      "epricipio, lo a la parte, de gorto; ni\n",
      "qué hizo Do\n",
      "================================================================================\n",
      "Validation set perplexity: 5.37\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 5100: 1.475024 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 5.30\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 5200: 1.474740 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 5.30\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 5300: 1.476331 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 5.29\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 5400: 1.476236 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 5.29\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 5500: 1.477465 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 5.28\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 5600: 1.478459 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 5.29\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 5700: 1.478982 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 5.29\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 5800: 1.478976 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 5.28\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 5900: 1.481001 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 5.28\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 6000: 1.480616 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "================================================================================\n",
      "basas de traré le sin, en la historia, ni por mejor las suelos,\n",
      "así tú\n",
      "quedares \n",
      "1\n",
      "andaría en los dos contrares decía, y hulque, y de cebasto, negrago -respondió\n",
      "Co que ranta; cuando openr\n",
      "con difinanta -respondió Sancho- y hablado ayado otro\n",
      "tados, que\n",
      "las manos i, un\n",
      "decejo.\n",
      "\n",
      "   ¿Valor los provecios,\n",
      "no me me horia le d\n",
      "tamente por\n",
      "que la\n",
      "mola.\n",
      "\n",
      "Digo Heros nos podré cortado de Tenos cuentas,\n",
      "como pe\n",
      "================================================================================\n",
      "Validation set perplexity: 5.27\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 6100: 1.479500 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 5.26\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 6200: 1.481985 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 5.27\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 6300: 1.483394 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 5.27\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 6400: 1.483231 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 5.26\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 6500: 1.483255 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 5.26\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 6600: 1.483363 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 5.26\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 6700: 1.483769 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 5.26\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 6800: 1.485012 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 5.25\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 6900: 1.486007 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 5.25\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 7000: 1.486199 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "================================================================================\n",
      "naba decir era luego,\n",
      "mas a mardiciona, y rodio que ¡onotro\n",
      "como yo si en proseg\n",
      "« En todos los que se había preguntara que promete a un\n",
      "hoy el vigo elos\n",
      "provito\n",
      "% aunque mostración a aquí persoro?; Don Porque si me servejino, comenzó \n",
      "  cual\n",
      "wo, da y retonas, allí nuestra, la nacida; y ajo en vuestra merced, llamaba en l\n",
      "Wó.\n",
      "Haque las pousías y, llegó a fuir mujado del por quiero las amos trabasbante\n",
      "================================================================================\n",
      "Validation set perplexity: 5.25\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 7100: 1.486911 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 5.23\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 7200: 1.488215 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 5.23\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 7300: 1.487113 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 5.22\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 7400: 1.486106 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 5.23\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 7500: 1.486745 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.23\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 7600: 1.486574 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 5.21\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 7700: 1.486563 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 5.21\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 7800: 1.485686 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 5.21\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 7900: 1.485027 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 5.22\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 8000: 1.485964 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "================================================================================\n",
      "Ñarsella el que las hija quedarán algaren dilerdima él la era la venta en nadua \n",
      "bie a mi alto, basta que con por escojeidas le suspidere, como seismo, acubieron\n",
      "o soy alguna otre Deonera de pasó deseo albía y otro,\n",
      "y peder.\n",
      "\n",
      "-Esto alama y\n",
      "m\n",
      "ero, abir no juvielle para que ya estaba le quiere ritaln así vuestra dellaver, \n",
      "Endonoso de venían. A comerme aquellas temeras\n",
      "-respicio\n",
      "del tiempo\n",
      "iguenta de h\n",
      "================================================================================\n",
      "Validation set perplexity: 5.22\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 8100: 1.485803 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 5.22\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 8200: 1.486249 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 5.22\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 8300: 1.485828 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 5.20\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 8400: 1.484650 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 5.20\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 8500: 1.485745 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 5.20\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 8600: 1.486504 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 5.20\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Average loss at step 8700: 1.487695 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 5.20\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 8800: 1.487396 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 5.19\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 8900: 1.489768 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 5.19\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 9000: 1.487834 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "================================================================================\n",
      ": da fombra, que las distifellas presentes, ni miser Duscilos que algunos; mijed\n",
      "arme, y pueero de el palabras; que más\n",
      "dosmeciendo, si al casto sobre án ser de\n",
      "E y\n",
      "mucho de gedido: ¿cuyo\n",
      "va andante le soho en los criodos, que parso\n",
      "los enti\n",
      "7aria y risvera así hay preguntó:\n",
      "\n",
      "Sayonde a la aloza, a los días: abáldeme que \n",
      "& que seis pegos se venía y comientes cuadras le de dos entre menos mía de la ma\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 9100: 1.488620 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 5.20\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 9200: 1.489659 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 5.20\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 9300: 1.488697 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 5.20\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 9400: 1.489168 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 5.19\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 9500: 1.488609 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 5.19\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Average loss at step 9600: 1.489517 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 5.18\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 9700: 1.488935 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 5.19\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 9800: 1.488863 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 5.19\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Average loss at step 9900: 1.488817 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 5.18\n",
      "Unexpected character: ï\n",
      "Unexpected character: à\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 10000: 1.489152 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.46\n",
      "================================================================================\n",
      "y\n",
      "escuderos piados mía volvie en el pason, En tiempo había lo mismo que se los q\n",
      "2 por don Figantor en destos los osquieros; que el aun tierra, vuestra merced ya\n",
      "ado, y a no\n",
      "le ha de paño en el aaber\n",
      "y ber un eljarse y de lo menos razón parec\n",
      "entes poya dicen el gobernado caballero de lo que\n",
      "recidiendela se un bastónon qu\n",
      "dodér; que dieran haberse invreviéndose unas manos\n",
      "y malellas mesmos de esto\n",
      "más\n",
      "================================================================================\n",
      "Validation set perplexity: 5.18\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 10100: 1.485088 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 5.18\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 10200: 1.483883 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 5.18\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 10300: 1.484533 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 5.18\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 10400: 1.496968 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 5.18\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 10500: 1.525158 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 5.18\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 10600: 1.538126 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.18\n",
      "Unexpected character: ï\n",
      "Average loss at step 10700: 1.537206 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 5.18\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Average loss at step 10800: 1.538002 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.18\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Average loss at step 10900: 1.536131 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 5.18\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Average loss at step 11000: 1.538134 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.65\n",
      "================================================================================\n",
      "« no lo ven a buena toma levante, el junto, siempre plato la virtud el duque la \n",
      "{ porque vieron dulcan habían de conocer en el di muerte. Ellos, si\n",
      "nos escuar, \n",
      "Toras con la pastora son por sus que se supie, el dar si mi acató: llegare compa\n",
      "^arme diga, que si sin dratazana en esto, no vivuría junto del barbero, y no le \n",
      "K En el mirad, ni quien tres cebidoco cogado si me desquiero de recibo, como tan\n",
      "================================================================================\n",
      "Validation set perplexity: 5.17\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 11100: 1.537951 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.17\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 11200: 1.538379 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.17\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 11300: 1.537866 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.17\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 11400: 1.539257 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.17\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 11500: 1.538139 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 5.17\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 11600: 1.538274 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.17\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 11700: 1.539130 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 5.17\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 11800: 1.539297 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 5.17\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 11900: 1.539475 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 5.17\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 12000: 1.539151 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.65\n",
      "================================================================================\n",
      "dos las de comenzas moda no se diese para más espretir afligntaron antarre arece\n",
      "{a y los pides brazos a venar su más españa tenerlo de la calar de mombre, Sanch\n",
      "Í que le tenía sobre no compareciendo así y pádiere, más, y el oave a la pal, co\n",
      "Satado que estaba ante no hija a Dios, tan pertor. De su ílganado no sus lacarle\n",
      "; y,\n",
      "porque este necesos, armas, otra rocura,\n",
      "cuando que le encaminar aquí de el\n",
      "================================================================================\n",
      "Validation set perplexity: 5.17\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 12100: 1.539367 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 5.17\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 12200: 1.540903 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 5.17\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 12300: 1.543084 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 5.16\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 12400: 1.542362 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 5.16\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 12500: 1.541566 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.16\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 12600: 1.542076 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 5.16\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 12700: 1.541999 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.16\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 12800: 1.541318 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 5.16\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 12900: 1.543131 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.15\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 13000: 1.542396 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "[ape.\n",
      "\n",
      "-Eso de la indijimas y\n",
      "vi vuestra galdeda desta vida\n",
      "recibir un mi largo \n",
      "^ dentro\n",
      "de la tierra. Y aquella\n",
      "cual trave la preberime de que puesto decir par\n",
      "uden, dicen y jurar en el dos demás, y\n",
      "con ellos libres gracias hay duque despoc\n",
      "&\n",
      "la sazón que no de la hica\n",
      "entre él cuatra su\n",
      "desvarréada; y si al bano, el ot\n",
      "0o que catencia, que Puente se príncipel de las años; que él terrí de réses don \n",
      "================================================================================\n",
      "Validation set perplexity: 5.15\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 13100: 1.540323 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 5.15\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 13200: 1.540959 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.15\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 13300: 1.540672 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 5.15\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Average loss at step 13400: 1.539307 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.15\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Average loss at step 13500: 1.539953 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 5.15\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Average loss at step 13600: 1.539973 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 5.15\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Average loss at step 13700: 1.540722 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 5.15\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Unexpected character: ï\n",
      "Average loss at step 13800: 1.540850 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 5.15\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 13900: 1.540725 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.15\n",
      "Unexpected character: ï\n",
      "Unexpected character: ï\n",
      "Unexpected character: ù\n",
      "Unexpected character: ù\n",
      "Average loss at step 14000: 1.540833 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.57\n",
      "================================================================================\n",
      "\n",
      "Sancho; que ellos blossaco de Acáptllen enzabademos,\n",
      "como debe; la puerte quisi\n",
      "que el ventero, doce de nadie más,\n",
      "sin ha buena lanza, para ser casario que no e\n",
      "Buonol que te lo parece, Sancho Panza; y comodida, y asiojo de mas; y llegó a ha\n",
      ": el injaron o caballero, y ajoso de la mano yo el rostir mis\n",
      "los discratos de l\n",
      "Í\n",
      "matorio, y por\n",
      "recatas se le como ya me sueles, porque dé arte, defende,. -Pue\n",
      "================================================================================\n",
      "Validation set perplexity: 5.15\n"
     ]
    }
   ],
   "source": [
    "num_steps = 14001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value Variable\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable)]]\n\nCaused by op 'Variable/read', defined at:\n  File \"/Users/william/anaconda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/william/anaconda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-379298b87110>\", line 11, in <module>\n    x[\"layer 1\"] = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 356, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 125, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2012, in identity\n    \"Identity\", input=input, name=name)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 789, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3018, in create_op\n    op_def=op_def)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1576, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/anaconda/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         c_api.TF_GetCode(status))\n\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7f75739f6a70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreset_sample_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0msample_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0msentence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcharacters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \"\"\"\n\u001b[0;32m--> 573\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4495\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4496\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4497\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1118\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1315\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1316\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable)]]\n\nCaused by op 'Variable/read', defined at:\n  File \"/Users/william/anaconda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/william/anaconda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-379298b87110>\", line 11, in <module>\n    x[\"layer 1\"] = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 356, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 125, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2012, in identity\n    \"Identity\", input=input, name=name)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 789, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3018, in create_op\n    op_def=op_def)\n  File \"/Users/william/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1576, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable\n\t [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable)]]\n"
     ]
    }
   ],
   "source": [
    "feed = sample(random_distribution())\n",
    "sentence = characters(feed)[0]\n",
    "reset_sample_state.run()\n",
    "for _ in range(300):\n",
    "  prediction = sample_prediction.eval({sample_input: feed})\n",
    "  feed = sample(prediction)\n",
    "  sentence += characters(feed)[0]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
